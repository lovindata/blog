{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/","title":"Master Scala Rest APIs in 3 Simple Concepts: Illustrated Guide with Tapir, http4s, and Circe!","text":"<p>What if I told you there's a game-changing Scala solution for building Rest APIs and generating SwaggerUI docs at lightning speed? Plus, it's as fast as FastAPI (for Python enthusiasts) to develop! All this while preserving the robust type safety and functional programming elegance of Scala! \ud83d\ude32 In this post, we'll dive deep into this exhilarating tech stack: Tapir, http4s, and Circe!</p> <p></p>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#why-tapir-http4s-circe","title":"\ud83d\ude0e Why Tapir, http4s, Circe?","text":"<p>Let's check out what is each library:</p> <ul> <li>Tapir: A powerful Scala library for defining and documenting HTTP APIs in a type-safe and functional manner, with built-in support for generating Swagger UI documentation.</li> <li>http4s: A lightweight, purely functional Scala library for building HTTP servers and clients, designed for high performance and composability.</li> <li>Circe: A popular JSON library for Scala that provides seamless JSON parsing and encoding using a powerful and idiomatic functional approach.</li> </ul> Handling Requests <p>As of now, in the Scala ecosystem, there are no actively maintained, production-ready alternatives to Tapir. However, alternatives to http4s include Play and Akka/Pekko Http, with Play being more a full-stack web framework. For JSON handling, alternatives to Circe include json4s, ZIO-json, and Play JSON. According to the 2023 Scala survey, it appears that the community is leaning towards the Typelevel ecosystem when it comes to building backend systems. You can also check library comparison websites like LibHunt for further insights.</p> Akka/Pekko ecosystem VS Typelevel ecosystem <p>Either way, all these alternatives are production-ready and actively maintained. You cannot go wrong with choosing any of them.</p> <p>For this series, the complete code is available on GitHub \ud83e\udee1 at https://github.com/lovindata/blog/tree/main/assets/posts/0/backend.</p>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#our-backend-castle","title":"\ud83c\udff0 Our Backend Castle!","text":"<p>Setting up a Scala project is essential, and in our case, we'll do it using IntelliJ. However, you can also opt for VSCode with Metals. If it is your first time setting up a Scala project, you can follow Scala's official tutorial \"Getting Started with Scala in IntelliJ\". The folder structure should be as follows:</p> <pre><code>backend/\n\u251c\u2500\u2500 project/\n\u2502   \u251c\u2500\u2500 build.properties\n\u2502   \u2514\u2500\u2500 plugins.sbt\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 main/\n\u2502       \u251c\u2500\u2500 resources/\n\u2502       \u2514\u2500\u2500 scala/\n\u2502           \u2514\u2500\u2500 Main.scala\n\u2514\u2500\u2500 build.sbt\n</code></pre> <p>In your <code>build.sbt</code>, please add the following dependencies:</p> build.sbt<pre><code>/**\n * Project configurations.\n */\nThisBuild / scalaVersion := \"2.13.12\" // https://www.scala-lang.org/download/all.html\nlazy val root = (project in file(\".\"))\n  .settings(name := \"backend\", idePackagePrefix := Some(\"com.lovindata\"), Defaults.itSettings)\n  .configs(IntegrationTest)\n\n/**\n * Dev dependencies.\n */\n// Cats &amp; Scala extensions\n// https://mvnrepository.com/artifact/org.typelevel/cats-effect\nlibraryDependencies += \"org.typelevel\" %% \"cats-effect\"        % \"3.5.3\"\n// https://github.com/oleg-py/better-monadic-for (look tags for version)\naddCompilerPlugin(\"com.olegpy\"         %% \"better-monadic-for\" % \"0.3.1\")\n// https://mvnrepository.com/artifact/dev.optics/monocle-core\nlibraryDependencies += \"dev.optics\"    %% \"monocle-core\"       % \"3.2.0\"\n\n// Tapir\n// https://mvnrepository.com/artifact/com.softwaremill.sttp.tapir/tapir-http4s-server\nlibraryDependencies += \"com.softwaremill.sttp.tapir\" %% \"tapir-http4s-server\"     % \"1.9.10\"\n// https://mvnrepository.com/artifact/com.softwaremill.sttp.tapir/tapir-json-circe\nlibraryDependencies += \"com.softwaremill.sttp.tapir\" %% \"tapir-json-circe\"        % \"1.9.10\"\n// https://mvnrepository.com/artifact/com.softwaremill.sttp.tapir/tapir-swagger-ui-bundle\nlibraryDependencies += \"com.softwaremill.sttp.tapir\" %% \"tapir-swagger-ui-bundle\" % \"1.9.10\"\n\n// Http4s\n// https://mvnrepository.com/artifact/org.http4s/http4s-ember-server\nlibraryDependencies += \"org.http4s\" %% \"http4s-ember-server\" % \"0.23.25\"\n// https://mvnrepository.com/artifact/org.http4s/http4s-circe\nlibraryDependencies += \"org.http4s\" %% \"http4s-circe\"        % \"0.23.25\"\n// https://mvnrepository.com/artifact/org.http4s/http4s-dsl\nlibraryDependencies += \"org.http4s\" %% \"http4s-dsl\"          % \"0.23.25\"\n// https://mvnrepository.com/artifact/org.http4s/http4s-ember-client\nlibraryDependencies += \"org.http4s\" %% \"http4s-ember-client\" % \"0.23.25\"\n\n// Circe\n// https://mvnrepository.com/artifact/io.circe/circe-parser\nlibraryDependencies += \"io.circe\" %% \"circe-parser\"         % \"0.14.6\"\n// https://mvnrepository.com/artifact/io.circe/circe-generic\nlibraryDependencies += \"io.circe\" %% \"circe-generic\"        % \"0.14.6\"\n// https://mvnrepository.com/artifact/io.circe/circe-generic-extras\nlibraryDependencies += \"io.circe\" %% \"circe-generic-extras\" % \"0.14.3\"\n// https://mvnrepository.com/artifact/io.circe/circe-literal\nlibraryDependencies += \"io.circe\" %% \"circe-literal\"        % \"0.14.6\"\n</code></pre> <p>You can run <code>Main.scala</code> by clicking on <code>\u25b7</code> and selecting <code>Run 'Main'</code>. If it works, you are all set! \ud83e\udd73</p>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#backend-electricity-setup","title":"\u26a1 Backend Electricity Setup","text":"<p>Let's build our first electric central that will empower the future bridges. A REST API server enables clients to interact with resources over HTTP using standard methods.</p> Backend Server Configuration <p>First setup our service which loads environment variables and our backend exceptions:</p> EnvLoaderConf.scala<pre><code>package com.lovindata\npackage config\n\nobject EnvLoaderConf {\n  private val allEnvVar: Map[String, String] = sys.env\n\n  val backendPort: Int = allEnvVar.getOrElse(\"BACKEND_PORT\", default = \"8080\").toInt\n}\n</code></pre> BackendException.scala<pre><code>package com.lovindata\npackage shared\n\nsealed trait BackendException extends Exception\n\nobject BackendException {\n  case class BadRequestException(message: String)          extends BackendException\n  case class ServerInternalErrorException(message: String) extends BackendException\n}\n</code></pre> <p>Second setup our backend server:</p> BackendServerConf.scala<pre><code>package com.lovindata\npackage config\n\nimport cats.effect.IO\nimport com.comcast.ip4s._\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.server.middleware.CORS\nimport shared.BackendException.ServerInternalErrorException\n\nobject BackendServerConf {\n  def start: IO[Unit] = for {\n    port &lt;- IO.fromOption(Port.fromInt(envLoaderConf.backendPort))(\n              ServerInternalErrorException(s\"Not processable port number ${envLoaderConf.backendPort}.\"))\n    _    &lt;- EmberServerBuilder\n              .default[IO]\n              .withHost(ipv4\"0.0.0.0\") // Accept connections from any available network interface\n              .withPort(port)          // On port 8080\n              .build\n              .use(_ =&gt; IO.never)\n              .start\n              .void\n  } yield ()\n}\n</code></pre> <p>Let's finally build the switch to turn on the electricity! \ud83d\udca1</p> Main.scala<pre><code>package com.lovindata\n\nimport cats.effect._\nimport config.BackendServerConf\n\nobject Main extends IOApp.Simple {\n  override def run: IO[Unit] = BackendServerConf.start &gt;&gt; IO.never // == non-terminating\n}\n</code></pre> <p>If you visit http://localhost:8080 and see the message <code>Not found</code>, then congratulations! You've built your electric central! \ud83e\udd73</p>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#first-bridge-endpoint","title":"\ud83c\udf09 First Bridge Endpoint!","text":"<p>It's now the exciting part because we are gonna connect to the external world with bridges! \ud83e\udd29 Endpoints are the specific URLs or routes in a web API that clients use to access and interact with resources or services.</p> Backend Endpoint <p>First, implement the business logic, which in our case involves counting the number of characters from a text.</p> TextSvc.scala<pre><code>package com.lovindata\npackage features.text\n\nimport cats.effect.IO\nimport cats.implicits._\n\nobject TextSvc {\n  def countCharacters(text: String): IO[Int] = text.length.pure[IO]\n}\n</code></pre> <p>Second, build the bridge! \ud83c\udf09\u2692\ufe0f</p> TextCtrl.scala<pre><code>package com.lovindata\npackage features.text\n\nimport cats.effect.IO\nimport org.http4s.HttpRoutes\nimport sttp.tapir._\nimport sttp.tapir.json.circe._\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\n\nobject TextCtrl {\n  def endpoints: List[AnyEndpoint] = List(countCharactersEpt)\n  def routes: HttpRoutes[IO]       = countCharactersRts\n\n  private val countCharactersEpt = endpoint // The endpoint and it is used to generate the OpenAPI doc\n    .summary(\"Count characters\")\n    .get\n    .in(\"count-characters\" / query[String](\"text\"))\n    .out(jsonBody[Int])\n  private val countCharactersRts =          // It converts the endpoint to actual http4s route :O\n    Http4sServerInterpreter[IO]().toRoutes(countCharactersEpt.serverLogicSuccess(text =&gt; TextSvc.countCharacters(text)))\n}\n</code></pre> <p>Lastly, connect our new, fresh \ud83c\udf09 bridge to our electrical central \u26a1\ufe0f\u2014you know, the one we built in the previous part. \ud83e\udd17</p> BackendServerConf.scala<pre><code>package com.lovindata\npackage config\n\nimport cats.effect.IO\nimport cats.implicits._\nimport com.comcast.ip4s._\nimport features.text.TextCtrl\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.implicits._\nimport shared.BackendException.ServerInternalErrorException\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\nimport sttp.tapir.swagger.bundle.SwaggerInterpreter\n\nobject BackendServerConf {\n  def start: IO[Unit] = for {\n    port &lt;- IO.fromOption(Port.fromInt(EnvLoaderConf.backendPort))(\n              ServerInternalErrorException(s\"Not processable port number ${EnvLoaderConf.backendPort}.\"))\n    _    &lt;- EmberServerBuilder\n              .default[IO]\n              .withHost(ipv4\"0.0.0.0\")        // Accept connections from any available network interface\n              .withPort(port)                 // On port 8080\n              .withHttpApp(allRts.orNotFound) // Link all routes to the backend server\n              .build\n              .use(_ =&gt; IO.never)\n              .start\n              .void\n  } yield ()\n\n  private val docsEpt = // Merge all endpoints as a fully usable OpenAPI doc\n    SwaggerInterpreter().fromEndpoints[IO](TextCtrl.endpoints, \"Backend\", \"1.0\")\n  private val allRts  = // Serve the OpenAPI doc &amp; all the other routes\n    Http4sServerInterpreter[IO]().toRoutes(docsEpt) &lt;+&gt; TextCtrl.routes\n}\n</code></pre> <p>If you visit http://localhost:8080/docs and end up on a SwaggerUI page, then congratulations! You've built your first bridge! \ud83e\udd73 You can play a little bit with your endpoint! \ud83d\ude04</p> SwaggerUI"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#mastering-circe-tapir","title":"\ud83c\udfa8 Mastering Circe &amp; Tapir","text":"<p>JSON is the go-to format when it comes to friendly chats between clients and servers. For example, imagine a frontend Single Page Application (SPA) running on a user's laptop communicating with a backend RestAPI. In this article, let's dive into the joy of decoding JSON data from the outside world into Scala classes, or encoding Scala classes into JSON to share with the world. \ud83c\udf10\u2728</p> Handling Requests"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#auto-derivation-magic","title":"\u2728 Auto Derivation Magic!","text":""},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#theory","title":"Theory","text":"<p>\"\u2728 Auto Derivation Magic!\" is the technique used to effortlessly convert JSON to Scala case classes or vice versa, leveraging the attributes of the case classes as JSON fields. For instance, consider the following JSON object:</p> <pre><code>{\n  \"id\": 1,\n  \"name\": \"James\",\n  \"gender\": \"male\",\n  \"age\": 26,\n  \"job\": \"software engineer\"\n}\n</code></pre> <p>The corresponding Scala case class, utilizing auto-derivation, would look like:</p> <pre><code>import features.guest.GuestMod.GenderEnum.Gender\n\ncase class GuestMod(\n    id: Long,\n    name: String,\n    gender: Gender,\n    age: Int,\n    job: String\n)\n\nobject GuestMod {\n  object GenderEnum extends Enumeration {\n    type Gender = Value\n    val Male: Value      = Value(\"male\")\n    val Female: Value    = Value(\"female\")\n    val NonBinary: Value = Value(\"non-binary\")\n  }\n}\n</code></pre> <p>As you can see, it's all about leveraging \"the case class attributes as JSON fields\". Importantly, to enable auto-derivation, you'll need to have an instance object of <code>io.circe.generic.AutoDerivation</code> and <code>sttp.tapir.generic.auto.SchemaDerivation</code> imported into scope. But don't worry, we'll delve into this in the practical part.</p>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#practice","title":"Practice","text":"\u2728 Auto derivation in action! <p>First, let's define the classes and methods required:</p> GuestMod.scala<pre><code>package com.lovindata\npackage features.guest\n\nimport features.guest.GuestMod.GenderEnum.Gender\nimport features.guest.dto.GuestDto\n\ncase class GuestMod( // Returned by the endpoints == \"Scala -&gt; JSON\" (also corresponds to an entity in table)\n    id: Long,\n    name: String,\n    gender: Gender,  // A non Scala simple type that needs to be derived manually! (JSON &lt;-&gt; Scala)\n    age: Int,\n    job: String)\n\nobject GuestMod {\n  def buildFromDto(id: Long, dto: GuestDto): GuestMod = GuestMod(id, dto.name, dto.gender, dto.age, dto.job)\n\n  object GenderEnum extends Enumeration {\n    type Gender = Value\n    val Male: Value      = Value(\"male\")\n    val Female: Value    = Value(\"female\")\n    val NonBinary: Value = Value(\"non-binary\")\n  }\n}\n</code></pre> GuestDto.scala<pre><code>package com.lovindata\npackage features.guest.dto\n\nimport features.guest.GuestMod.GenderEnum.Gender\n\ncase class GuestDto( // It corresponds to the input of the endpoint (JSON -&gt; Scala)\n    name: String,\n    gender: Gender,\n    age: Int,\n    job: String)\n</code></pre> <p>Secondly, the shortest but most crucial step! \u26a0\ufe0f By importing this <code>Serializers</code> object into scope, it implies that all case classes will be convertible between \"JSON \u2194 Scala\". \ud83e\udd2f</p> Serializers.scala<pre><code>package com.lovindata\npackage features.shared\n\nimport features.guest.GuestMod.GenderEnum\nimport features.guest.GuestMod.GenderEnum.Gender\nimport io.circe._\nimport io.circe.generic.AutoDerivation\nimport sttp.tapir.Schema\nimport sttp.tapir.generic.auto.SchemaDerivation\n\nobject Serializers extends AutoDerivation with SchemaDerivation { // HERE! \u2728 Auto Derivation Magic!\n  // Enumeration needs to be auto derived manually with theses 3 lines \ud83d\udc47 (It will use enumeration actual values when (en/de)coding)\n  implicit val genderEnc: Encoder[Gender] = Encoder.encodeEnumeration(GenderEnum)\n  implicit val genderDec: Decoder[Gender] = Decoder.decodeEnumeration(GenderEnum)\n  implicit val genderSch: Schema[Gender]  = Schema.derivedEnumerationValue[Gender]\n}\n</code></pre> <p>Thirdly, let's address the repository responsible for managing the <code>GuestMod</code> table and the associated business logic:</p> GuestRep.scala<pre><code>package com.lovindata\npackage features.guest\n\nimport cats.effect._\nimport cats.effect.unsafe.implicits._\nimport features.guest.dto.GuestDto\n\nobject GuestRep { // This layer is not important. It's an in-memory table for the example to work.\n  def insert(dto: GuestDto): IO[GuestMod] = guestsTable.modify { table =&gt;\n    val id    = table.length\n    val guest = GuestMod.buildFromDto(id, dto)\n    (table :+ guest, guest) // (Updated table, Returned class)\n  }\n\n  def list(): IO[Vector[GuestMod]] = guestsTable.get\n\n  private val guestsTable: Ref[IO, Vector[GuestMod]] =\n    Ref[IO].of(Vector.empty[GuestMod]).unsafeRunSync() // A concurrent safe in memory table\n}\n</code></pre> GuestSvc.scala<pre><code>package com.lovindata\npackage features.guest\n\nimport cats.effect.IO\nimport features.guest.dto.GuestDto\nimport shared.BackendException.BadRequestException\n\nobject GuestSvc {\n  def letEnterAdultGuest(dto: GuestDto): IO[GuestMod] = for {\n    _     &lt;- IO.raiseUnless(dto.age &gt;= 18)(\n               BadRequestException(\"You are not an adult!\") // Exception of \"BadRequestException\" raised\n             )\n    guest &lt;- GuestRep.insert(dto)\n  } yield guest\n\n  def listGuests(): IO[Vector[GuestMod]] = GuestRep.list()\n}\n</code></pre> <p>Fourth, let's define our \ud83c\udf09 bridges (endpoints) that will allow guests to enter the castle \ud83c\udff0 and monitor their activities (\"see ya, personal privacy\" \ud83d\udc40\ud83d\ude08).</p> GuestCtrl.scala<pre><code>package com.lovindata\npackage features.guest\n\nimport cats.effect.IO\nimport cats.implicits._\nimport features.guest.dto.GuestDto\nimport features.shared.Serializers._ // \u2728 Auto Derivation Magic imported in scope! (== Make Scala case classes \"JSON \u2194 Scala\" convertible)\nimport org.http4s.HttpRoutes\nimport shared.BackendException.BadRequestException\nimport sttp.model.StatusCode\nimport sttp.tapir._\nimport sttp.tapir.json.circe._\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\n\nobject GuestCtrl {\n  def endpoints: List[AnyEndpoint] = List(letEnterAdultGuestEpt, listGuestsEpt)\n  def routes: HttpRoutes[IO]       = letEnterAdultGuestRts &lt;+&gt; listGuestsRts\n\n  private val letEnterAdultGuestEpt = endpoint\n    .summary(\"Let enter adult guest\")\n    .post\n    .in(\"guests\")\n    .in(jsonBody[GuestDto])                 // \u2728 Auto Derivation Magic applied! (Not just simple type but case class this time)\n    .out(jsonBody[GuestMod])                // \u2728 Auto Derivation Magic applied!\n    .errorOut(\n      statusCode(StatusCode.BadRequest)\n        .and(jsonBody[BadRequestException]) // This endpoint can throw errors + \u2728 Auto Derivation Magic applied!\n    )\n  private val letEnterAdultGuestRts = Http4sServerInterpreter[IO]().toRoutes(\n    letEnterAdultGuestEpt\n      .serverLogicRecoverErrors( // == recover from \"BadRequestException\" exceptions raised + Encode as JSON and return them\n        dto =&gt; GuestSvc.letEnterAdultGuest(dto)))\n\n  private val listGuestsEpt = endpoint\n    .summary(\"List guests\")\n    .get\n    .in(\"guests\")\n    .out(\n      jsonBody[\n        Vector[\n          GuestMod // /!\\ Vector of Scala case class derivable is also derivable == \u2728 Auto Derivation Magic applied!\n        ]\n      ]\n    )\n  private val listGuestsRts =\n    Http4sServerInterpreter[IO]().toRoutes(listGuestsEpt.serverLogicSuccess(_ =&gt; GuestSvc.listGuests()))\n}\n</code></pre> <p>Finally, as done in the previous section, let's \"connect the bridges to the electrical central \ud83c\udf09 \u2194 \u26a1\".</p> BackendServerConf.scala<pre><code>package com.lovindata\npackage config\n\nimport cats.effect.IO\nimport cats.implicits._\nimport com.comcast.ip4s._\nimport features.guest.GuestCtrl\nimport features.text.TextCtrl\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.implicits._\nimport shared.BackendException.ServerInternalErrorException\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\nimport sttp.tapir.swagger.bundle.SwaggerInterpreter\n\nobject BackendServerConf {\n  def start: IO[Unit] = for {\n    port &lt;- IO.fromOption(Port.fromInt(EnvLoaderConf.backendPort))(\n              ServerInternalErrorException(s\"Not processable port number ${EnvLoaderConf.backendPort}.\"))\n    _    &lt;- EmberServerBuilder\n              .default[IO]\n              .withHost(ipv4\"0.0.0.0\")        // Accept connections from any available network interface\n              .withPort(port)                 // On port 8080\n              .withHttpApp(allRts.orNotFound) // Link all routes to the backend server\n              .build\n              .use(_ =&gt; IO.never)\n              .start\n              .void\n  } yield ()\n\n  private val docsEpt = // Merge all endpoints as a fully usable OpenAPI doc\n    SwaggerInterpreter().fromEndpoints[IO](TextCtrl.endpoints ++ GuestCtrl.endpoints, \"Backend\", \"1.0\")\n  private val allRts  = // Serve the OpenAPI doc &amp; all the other routes\n    Http4sServerInterpreter[IO]().toRoutes(docsEpt) &lt;+&gt; TextCtrl.routes &lt;+&gt; GuestCtrl.routes\n}\n</code></pre>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#results","title":"Results","text":"<p>After re-running, navigate to http://localhost:8080/docs to interact with your new endpoints! \ud83d\ude0a</p> New guest success New guest failed List guests"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#crack-adts","title":"\ud83e\uddd9\u200d\u2642\ufe0f Crack ADTs!","text":""},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#theory_1","title":"Theory","text":"<p>Algebraic Data Types (ADTs) are a way of defining data structures by combining simpler types through two main constructs: Sum Types, representing a choice between alternatives, and Product Types, representing a combination of different types.</p> <p>Blablabla \ud83e\udd2a, remember that it's commonly just a sealed trait, like this for example (refer to comments if you want to understand in detail):</p> <pre><code>sealed trait Pet // It's an ADT because \"Product types\" and \"Sum types\"\nobject Pet {\n  case class Dog(name: String,\n                  age: Int) // \ud83d\udc48 \"Product types\" because \"(String, Int)\" == More complex type by combining types\n      extends Pet // \"Sum types\" because \"Dog != Cat != Fish != Bird but Dog + Cat + Fish + Bird = Pet\" == Distinct types when united define the type\n  case class Cat(name: String, age: Int)  extends Pet\n  case class Fish(name: String, age: Int) extends Pet\n  case class Bird(name: String, age: Int) extends Pet\n}\n</code></pre> <p>Now, what if I tell you there's a way to apply the \"\u2728 Auto Derivation Magic!\" to sealed traits using a discriminator field? \ud83e\udd2f For example, these two can be encoded or decoded from one to another:</p> <pre><code>[\n  {\n    \"type\": \"Dog\", // \u26a0\ufe0f Discriminator field\n    \"name\": \"Max\",\n    \"age\": 3\n  },\n  {\n    \"type\": \"Fish\", // \u26a0\ufe0f Discriminator field\n    \"name\": \"Poissy (\u2190 RER A \ud83e\udd2a, don't worry if you don't understand, Parisians will! XDD)\",\n    \"age\": 2\n  },\n  {\n    \"type\": \"Dog\", // \u26a0\ufe0f Discriminator field\n    \"name\": \"Doggy\",\n    \"age\": 1\n  }\n]\n</code></pre> <pre><code>val pets: Vector[Pet] = Vector(\n  Dog(name = \"Max\", age = 3),\n  Fish(name = \"Poissy (\u2190 RER A \\uD83E\\uDD2A, don't worry if you don't understand, Parisians will! XDD)\", age = 2),\n  Dog(name = \"Doggy\", age = 1)\n)\n</code></pre> <p>For this to work, only two things needed:</p> <ul> <li>An instance object of <code>io.circe.generic.extras.AutoDerivation</code> and <code>sttp.tapir.generic.auto.SchemaDerivation</code> imported into scope (\u26a0\ufe0f switch <code>io.circe.generic.AutoDerivation</code> to <code>io.circe.generic.extras.AutoDerivation</code>)</li> <li>Additionally, two implicit variables of <code>io.circe.generic.extras.Configuration</code> and <code>sttp.tapir.generic.Configuration</code> imported into scope (for the discriminator field)</li> </ul> <p>Bear with me \ud83d\ude4f, you will understand in the practice part \ud83d\udc4d!</p> <p>Note</p> <p>By default, I recommend always opting for the ADT configuration mentioned above. This is because the ADT configuration supports both standalone case classes and case classes with sealed traits, covering our previous use case seamlessly and providing support for ADTs for free.</p>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#practice_1","title":"Practice","text":"ADT decoded and encoded (Pet contest) <p>First, let's define the sealed trait and the necessary business logic:</p> PetDto.scala<pre><code>package com.lovindata\npackage features.pet.dto\n\nsealed trait PetDto // An ADT\n\nobject PetDto {\n  case class Dog(name: String, age: Int)  extends PetDto\n  case class Cat(name: String, age: Int)  extends PetDto\n  case class Fish(name: String, age: Int) extends PetDto\n  case class Bird(name: String, age: Int) extends PetDto\n}\n</code></pre> PetSvc.scala<pre><code>package com.lovindata\npackage features.pet\n\nimport cats.effect.IO\nimport features.pet.dto.PetDto\nimport features.pet.dto.PetDto.Dog\nimport shared.BackendException.BadRequestException\n\nobject PetSvc {\n  def petContest(dto: Vector[PetDto]): IO[PetDto] = for {\n    _      &lt;- IO.raiseUnless(dto.nonEmpty)(BadRequestException(\"Where are the pets ?! \ud83d\ude21\"))\n    pets    = dto.sortWith {\n                case (pet0: Dog, pet1: Dog) =&gt; pet0.age &lt; pet1.age // Sort before the younger dog\n                case (_: Dog, _)            =&gt; true                // Sort before the dog\n                case _                      =&gt; false\n              }\n    bestPet = pets.head\n  } yield bestPet\n}\n</code></pre> <p>Secondly, the most crucial part, but just 3 lines of code! \ud83e\udd23 The switch import and the two additional implicits.</p> Serializers.scala<pre><code>package com.lovindata\npackage features.shared\n\nimport features.guest.GuestMod.GenderEnum\nimport features.guest.GuestMod.GenderEnum.Gender\nimport io.circe._\nimport io.circe.generic.extras.AutoDerivation // \u26a0\ufe0f Switched!\nimport io.circe.generic.extras.Configuration\nimport sttp.tapir.Schema\nimport sttp.tapir.generic.{Configuration =&gt; TapirConfiguration}\nimport sttp.tapir.generic.auto.SchemaDerivation\n\nobject Serializers extends AutoDerivation with SchemaDerivation { // HERE! \u2728 Auto Derivation Magic!\n  // The 2 implicits \ud83d\udc47\n  implicit val encDecConf: Configuration   = Configuration.default.withDiscriminator(\"type\")\n  implicit val schConf: TapirConfiguration = TapirConfiguration.default.withDiscriminator(\"type\")\n\n  // Enumeration needs to be auto derived manually with theses 3 lines \ud83d\udc47 (It will use enumeration actual values when (en/de)coding)\n  implicit val genderEnc: Encoder[Gender] = Encoder.encodeEnumeration(GenderEnum)\n  implicit val genderDec: Decoder[Gender] = Decoder.decodeEnumeration(GenderEnum)\n  implicit val genderSch: Schema[Gender]  = Schema.derivedEnumerationValue[Gender]\n}\n</code></pre> <p>Thirdly, let's define our endpoint.</p> PetCtrl.scala<pre><code>package com.lovindata\npackage features.pet\n\nimport cats.effect.IO\nimport features.pet.dto.PetDto\nimport features.shared.Serializers._ // \u2728 Auto Derivation Magic + 2 implicits imported in scope! (== Make Scala sealed traits \"JSON \u2194 Scala\" convertible)\nimport org.http4s.HttpRoutes\nimport shared.BackendException.BadRequestException\nimport sttp.model.StatusCode\nimport sttp.tapir._\nimport sttp.tapir.json.circe._\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\n\nobject PetCtrl {\n  def endpoints: List[AnyEndpoint] = List(petContestEpt)\n  def routes: HttpRoutes[IO]       = petContestRts\n\n  private val petContestEpt = endpoint\n    .summary(\"Pet contest\")\n    .post\n    .in(\"pets\" / \"contest\")\n    .in(jsonBody[Vector[PetDto]])                                                   // \u2728 Auto Derivation Magic applied! (A sealed trait this time :O)\n    .out(jsonBody[PetDto])                                                          // \u2728 Auto Derivation Magic applied!\n    .errorOut(statusCode(StatusCode.BadRequest).and(jsonBody[BadRequestException])) // \u2728 Auto Derivation Magic applied!\n  private val petContestRts =\n    Http4sServerInterpreter[IO]().toRoutes(petContestEpt.serverLogicRecoverErrors(dto =&gt; PetSvc.petContest(dto)))\n}\n</code></pre> <p>Finally, you know the tune now, \"\ud83c\udf09 \u2194 \u26a1\".</p> BackendServerConf.scala<pre><code>package com.lovindata\npackage config\n\nimport cats.effect.IO\nimport cats.implicits._\nimport com.comcast.ip4s._\nimport features.guest.GuestCtrl\nimport features.pet.PetCtrl\nimport features.text.TextCtrl\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.implicits._\nimport shared.BackendException.ServerInternalErrorException\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\nimport sttp.tapir.swagger.bundle.SwaggerInterpreter\n\nobject BackendServerConf {\n  def start: IO[Unit] = for {\n    port &lt;- IO.fromOption(Port.fromInt(EnvLoaderConf.backendPort))(\n              ServerInternalErrorException(s\"Not processable port number ${EnvLoaderConf.backendPort}.\"))\n    _    &lt;- EmberServerBuilder\n              .default[IO]\n              .withHost(ipv4\"0.0.0.0\")        // Accept connections from any available network interface\n              .withPort(port)                 // On port 8080\n              .withHttpApp(allRts.orNotFound) // Link all routes to the backend server\n              .build\n              .use(_ =&gt; IO.never)\n              .start\n              .void\n  } yield ()\n\n  private val docsEpt = // Merge all endpoints as a fully usable OpenAPI doc\n    SwaggerInterpreter()\n      .fromEndpoints[IO](TextCtrl.endpoints ++ GuestCtrl.endpoints ++ PetCtrl.endpoints, \"Backend\", \"1.0\")\n  private val allRts  = // Serve the OpenAPI doc &amp; all the other routes\n    Http4sServerInterpreter[IO]().toRoutes(docsEpt) &lt;+&gt; TextCtrl.routes &lt;+&gt; GuestCtrl.routes &lt;+&gt; PetCtrl.routes\n}\n</code></pre>"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#results_1","title":"Results","text":"Pet contest"},{"location":"2024/02/21/master-scala-rest-apis-in-3-simple-concepts-illustrated-guide-with-tapir-http4s-and-circe/#happy-endings","title":"\ud83c\udf1f Happy Endings!","text":"<p>If you've made it this far, you should now be able to handle 80% of overall backend requirements \ud83d\udcaa, specifically \"friendly chat between clients and servers using JSON\". I'm counting on you \ud83d\ude4f, and remember \ud83e\udd7a, there are only three concepts to keep in mind:</p> <ul> <li>\u26a1 Backend server</li> </ul> <pre><code>import com.comcast.ip4s._\nimport org.http4s.ember.server.EmberServerBuilder\n\nEmberServerBuilder\n  .default[IO]\n  .withHost(ipv4\"0.0.0.0\")\n  .withPort(myPort)\n  .withHttpApp(allRts.orNotFound)\n  .build\n  .use(_ =&gt; IO.never)\n  .start\n  .void\n</code></pre> <ul> <li>\ud83c\udf09 Endpoint and Route</li> </ul> <pre><code>import sttp.model.StatusCode\nimport sttp.tapir._\nimport sttp.tapir.json.circe._\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\n\nprivate val myEpt = endpoint\n  .summary(\"My endpoint summary\")\n  .post\n  .in(\"my-endpoint-path\")\n  .in(jsonBody[???])\n  .out(jsonBody[???])\n  .errorOut(statusCode(StatusCode.???).and(jsonBody[???]))\nprivate val myRts =\n  Http4sServerInterpreter[IO]().toRoutes(myEpt.serverLogicRecoverErrors(inputs =&gt; ???))\n</code></pre> <ul> <li>\u2728 Auto Derivation Magic!</li> </ul> <pre><code>import io.circe._\nimport io.circe.generic.extras.AutoDerivation\nimport io.circe.generic.extras.Configuration\nimport sttp.tapir.Schema\nimport sttp.tapir.generic.{Configuration =&gt; TapirConfiguration}\nimport sttp.tapir.generic.auto.SchemaDerivation\n\nobject Serializers extends AutoDerivation with SchemaDerivation {\n  implicit val encDecConf: Configuration   = Configuration.default.withDiscriminator(\"type\")\n  implicit val schConf: TapirConfiguration = TapirConfiguration.default.withDiscriminator(\"type\")\n\n  // ... some other encoders, decoders and schemas that needs to be defined manually (like enums)\n}\n</code></pre> <p>The remaining 20% involves setting up security, streaming support, web sockets, server-sent events, and serving static content. You can find more information about these topics on the Tapir official docs. In most cases, these only need to be set up once or may not be necessary. However, if you'd like me to cover them, please let me know! You can reach me on social media by sending a message \ud83e\udd17!</p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/","title":"Image Super-Resolution: DAT (Dual Aggregation Transformer) the new goat for Pythonistas? \ud83d\udc0d","text":"<p>As a Python aficionado, I sought to leverage my programming acumen to enhance cherished images. Armed with the powerful Pillow library, I embarked on this mission, only to be met with disappointment \ud83d\ude11 when the results from the conventional <code>resize</code> method yielded subpar quality \u2013 a far cry from the vibrant images I envisioned. In this article, I am excited to unveil a groundbreaking solution. Say goodbye to lackluster results and hello to superior image quality! \ud83e\udee8</p> <p></p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#why-dat","title":"\ud83e\udd14 Why DAT?","text":"<p>Image super resolution and bicubic interpolation differ in their methods for improving image quality. Bicubic interpolation is a simple algorithm that adjusts pixel values to resize images, often leading to blurry outcomes when enlarging. Conversely, image super resolution, powered by deep learning models like CNNs, aims to create high-resolution images with enhanced details by learning from high-quality data.</p> <p>DAT stands as one of the foremost image super-resolution algorithms. Presently, accessible and user-friendly open-source alternatives encompass:</p> <ul> <li>OpenCV's models (EDSR, ESPCN, FSRCNN, LapSRN) from <code>opencv-contrib-python-headless</code> library</li> <li>ESRGAN from github.com/xinntao/ESRGAN</li> </ul> <p>These options necessitate an initial setup, like downloading weights. For ESRGAN, some code tinkering is needed to make it programmatically usable. Additionally, these alternatives are becoming dated \u2014 EDSR was released in 2017 and ESRGAN in 2018. The field of research in image super resolution has since progressed.</p> Image Super-Resolution on Set14 - 4x upscaling (paperswithcode.com) <p>As machine learning engineers, data scientists, or even Python developers, we always seek the best, newest, most versatile, user-friendly, and fastest models, don't we? So, let's kick things off with DAT! \ud83d\ude0e</p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#how-to-use-dat","title":"\ud83d\ude80 How to use DAT?","text":""},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#quickstart-upscale","title":"Quickstart: <code>upscale</code>","text":"<p>For the installation according to your dependency manager:</p> PyPI<pre><code>pip install pillow-dat\n</code></pre> Poetry<pre><code>poetry add pillow-dat\n</code></pre> <p>For the usage:</p> example.py<pre><code>from PIL.Image import open\n\nfrom PIL_DAT.Image import upscale\n\nlumine_image = open(\".github/lumine.png\")\nlumine_image = upscale(lumine_image, 2)\n</code></pre> <p>As you can see, DAT is usable through <code>pillow-dat</code>, a Pillow library extension. It provides an upscale of times 2, 3 and 4. But the best of all, it's just one line of code \ud83d\ude0e.</p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#advanced-custom-models","title":"Advanced: custom models","text":"<p>The library offers four versions of DAT models for advanced programmers. Here's an example for a scaling factor of 4:</p> <ul> <li>DAT light with 573K parameters</li> <li>DAT S with 11.21M parameters</li> <li>DAT 2 with 11.21M parameters</li> <li>DAT with 14.80M parameters</li> </ul> <p>By default, the method <code>PIL_DAT.Image.upscale</code> utilizes the embarked DAT light models. However, if you're feeling adventurous or require even higher image quality, you can access these advanced versions using custom models:</p> example_custom_model.py<pre><code>from PIL.Image import open\n\nfrom PIL_DAT.dat_s import DATS\n\nlumine_image = open(\".github/lumine.png\")\nmodel = DATS(\"./dist/DAT_S_x4.pth\", 4)  # Instantiate a reusable custom model instance\nlumine_image = model.upscale(lumine_image)\nlumine_image.show()\n</code></pre> <p>Please note that model weights in <code>*.pth</code> format are accessible via a Google Drive link provided on GitHub or PyPI.</p> <p>By default, when you use the <code>PIL_DAT.Image.upscale</code> method, it loads the model, performs the upscaling, and clears the model from the RAM for you. For better performance, especially when calling this function multiple times for the same scaling factor, it's recommended to instantiate the DAT light model via custom models.</p> <pre><code>from PIL.Image import open\n\nfrom PIL_DAT.dat_light import DATLight\n\nlumine_image = open(\".github/lumine.png\")\nmodel = DATLight(2)  # Instantiate a reusable DATLight custom model instance\nlumine_image = model.upscale(lumine_image)\nlumine_image.show()\n</code></pre>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#benchmarks","title":"\ud83d\udcca Benchmarks","text":"<p>DAT will be compared to OpenCV's top super resolution model, EDSR, and a commercial SaaS product, Img.Upscaler.</p> <p>All benchmark results presented here are reproducible. For detailed implementation, please consult the following resources:</p> <ul> <li>Located in the benchmarks folder on the official GitHub repository of <code>pillow-dat</code>.</li> <li>Within the scripts folder that houses our personal source code, used for testing this library.</li> </ul>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#speed","title":"Speed","text":"<p>Performance benchmarks have been conducted on a computing system equipped with an Intel(R) CORE(TM) i7-9750H CPU @ 2.60GHz processor, accompanied by a 2 \u00d7 8 Go at 2667MHz RAM configuration. Below are the recorded results:</p> In seconds 320 \u00d7 320 640 \u00d7 640 960 \u00d7 960 1280 \u00d7 1280 DAT light (x2) 16.1 65.3 146.8 339.8 DAT light (x3) 14.3 61.7 - - DAT light (x4) 14.0 63.0 - - <p>The results were compared against the renowned <code>OpenCV</code> library, utilizing its <code>EDSR</code> model known for delivering superior image quality.</p> In seconds 320 \u00d7 320 640 \u00d7 640 960 \u00d7 960 1280 \u00d7 1280 EDSR (x2) 25.6 112.9 264.1 472.8 EDSR (x3) 24.3 112.5 - - EDSR (x4) 23.6 111.2 - - <p>Note: Since we don't have control over Img.Upscaler's hardware specifications, it's challenging to provide accurate speed benchmarks.</p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#quality","title":"Quality","text":"<p>Let's delineate five key themes encompassing all image types: Abstract, Animal, Nature, Object, and People. Subsequently, we will employ each solution to upscale images under each respective theme.</p> Abstract Animal Nature Object People <p>While the images may initially seem identical, closer inspection or zooming in reveals subtle quality differences. Let's examine the zoomed images for a clearer view! \ud83d\udd0e</p> Abstract Zoomed Animal Zoomed Nature Zoomed Object Zoomed People Zoomed <p>While informative, these tests may not encompass every scenario for each solution. Your images may vary, impacting results. For the most reliable assessment, try each solution yourself to form your own opinion! \ud83d\ude09</p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#alpha-channel-awareness","title":"Alpha-channel-awareness","text":"<p>State-of-the-art super-resolution models typically only support RGB images, and this holds true for the original DAT models as well. The reason behind this is that datasets used for comparing models in research predominantly consist of RGB images. As a user, this can pose a challenge. However, <code>pillow-dat</code> offers a solution with its built-in post-processing logic, effortlessly handling any alpha channel for you! \ud83c\udf1f</p> Alpha-channel-awareness <p>In this example, we're just comparing the basic usage of each solution. While it's possible to manage the alpha channel manually for the OpenCV's EDSR case, it would require additional effort.</p>"},{"location":"2024/03/17/image-super-resolution-dat-dual-aggregation-transformer-the-new-goat-for-pythonistas-/#conclusion","title":"\ud83d\udc51 Conclusion","text":"<p>This library is founded upon the pioneering research paper, \"Dual Aggregation Transformer for Image Super-Resolution\".</p> <pre><code>@inproceedings{chen2023dual,\n    title={Dual Aggregation Transformer for Image Super-Resolution},\n    author={Chen, Zheng and Zhang, Yulun and Gu, Jinjin and Kong, Linghe and Yang, Xiaokang and Yu, Fisher},\n    booktitle={ICCV},\n    year={2023}\n}\n</code></pre> <p>We extend our heartfelt appreciation to the researchers. The researchers' groundbreaking contributions have inspired the development of this library, pushing forward image super-resolution. \ud83d\ude4f</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/","title":"Introducing TARP Stack \u26fa \u2013 Tapir, React and PostgreSQL","text":"<p>I landed my first job as a Data Engineer using Scala. It's been over 3 years now, approaching 4 years. The more experience you gain, the more you want to spread your wings \ud83e\udebd to tackle even bigger and more complex projects than just data pipelines, like developing full-stack web data applications. But, I really do not want to dissipate myself too much on all the programming languages, libraries, or frameworks out there \ud83d\ude23. These are just tools. What's important is how efficiently you can use them for the product or feature you envisioned \ud83e\udd84\ud83c\udf08. Sooo! For me, it's currently the TARP tech stack!</p> <p></p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#what-is-tarp","title":"\ud83e\udd14 What is TARP?","text":"<p>TARP stands for Tapir, React and PostgreSQL. In detail:</p> <ul> <li>\ud83e\udd9b Tapir: For the backend, it's a lightweight library similar to FastAPI, designed for building endpoints and providing free SwaggerUI docs.</li> <li>\u269b\ufe0f React: For the frontend, it's the most popular framework with the largest community, according to the Stack Overflow Developer Survey 2023.</li> <li>\ud83d\udc18 PostgreSQL: Chosen for the database due to its popularity and strong community support, as indicated by the Stack Overflow Developer Survey 2023.</li> </ul> <p>I'm really excited \ud83d\ude04 to demonstrate how productive you can be with this tech stack. Let's start building, shall we! By the way, do you know what \"TARP\" stands for? \ud83e\udd23 A tarp functions as a waterproof protective cover, for example, when building tents \u26fa.</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#development-environment","title":"\ud83d\udc68\u200d\ud83d\udcbb Development Environment","text":"<p>For your coding environment, I highly recommend using VSCode. It has amazing support for TypeScript and Docker with various extensions. Scala development can also be done on VSCode using the Metals extension. I used to develop on IntelliJ, but got tired of switching between VSCode and IntelliJ \ud83d\ude2b. So yeah, if you have to handle more than just Scala code, just go with VSCode \ud83d\ude0b.</p> <p>Let's create 3 folders: <code>./devops</code>, <code>./backend</code>, <code>./frontend</code>, and also the <code>./vscode.code-workspace</code> file.</p> ./vscode.code-workspace<pre><code>{\n  \"folders\": [\n    {\n      \"path\": \"backend\"\n    },\n    {\n      \"path\": \"devops\"\n    },\n    {\n      \"path\": \"frontend\"\n    }\n  ]\n}\n</code></pre> <p>You've just organized your project into VSCode workspaces. This is a way to instruct VSCode to treat each folder as an independent workspace, allowing you to work on them simultaneously within a single VSCode window. When you open the <code>./vscode.code-workspace</code> file using VSCode, it will automatically detect three workspaces.</p> VSCode workspaces <p>The goal of this section is to establish a well-structured VSCode folder for your FullStack application as shown below \ud83d\udc47.</p> Folder structure <p>Let's start! \ud83d\ude24</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#database","title":"Database","text":"<p>The goal here is to set up a local PostgreSQL database and be able to explore it with suitable tools. This will be achieved using a PostgreSQL Docker container and the SQLTools VSCode extension. Please install:</p> <ul> <li>\ud83d\udc33 Docker Desktop: For setting up a local PostgreSQL database.</li> <li>\u2795 Docker VSCode extension: To execute Docker commands directly via the VSCode UI.</li> <li>\ud83d\udd0e SQLTools and SQLTools PostgreSQL/Cockroach Driver VSCode extensions: For viewing the local PostgreSQL database.</li> </ul> <p>Let's create a Docker Compose file, which is simply a YAML file with specific syntax, to run the local PostgreSQL database.</p> ./devops/dev-local/docker-compose.yml<pre><code>services:\n  database:\n    image: postgres:16.2\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_PASSWORD=tarp\n      - POSTGRES_USER=tarp\n      - POSTGRES_DB=tarp\n    volumes:\n      - ./data:/var/lib/postgresql/data # Optional, but can keep our database data persistent on the host disk.\n</code></pre> <p>You are now all set to run it:</p> <ul> <li>Right click on <code>./devops/dev-local/docker-compose.yml</code></li> <li>Click on <code>Compose Up</code></li> </ul> <p>After a little while, if you go to the Docker Desktop application, you should see your local PostgreSQL database running \ud83d\ude03!</p> PostgreSQL container <p>To ensure the local PostgreSQL setup is correct, you can explore it using the SQLTools extension in VSCode. To do this, add a new connection in SQLTools:</p> <ul> <li><code>CTRL + SHIFT + P</code></li> <li>Click on <code>SQLTools Management: Add New Connection</code></li> <li>Follow the instructions and fill in the fields according to how the PostgreSQL container is configured in <code>./devops/dev-local/docker-compose.yml</code></li> </ul> PostgreSQL view using SQLTools <p>If you've reached this point, your local PostgreSQL database is now all set for development! \ud83d\udc4d</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#backend","title":"Backend","text":"<p>Let's set up our Scala project! First, install:</p> <ul> <li>\u2615\ufe0f Java 17: Because Scala runs on top of the JVM.</li> <li>\u2699\ufe0f Metals VSCode extension: For supporting us during implementation in Scala.</li> </ul> <p>Then, two files must be defined:</p> <ul> <li><code>./backend/project/build.properties</code>: For specifying the SBT (the Scala dependencies manager) version.</li> </ul> ./backend/project/build.properties<pre><code># https://github.com/sbt/sbt (look tags for version)\nsbt.version=1.9.9\n</code></pre> <ul> <li><code>./backend/build.sbt</code>: For project metadata and dependencies.</li> </ul> ./backend/build.sbt<pre><code>/**\n * Project configurations.\n */\nThisBuild / scalaVersion := \"2.13.13\" // https://www.scala-lang.org/download/all.html\nlazy val root = (project in file(\".\")).settings(name := \"backend\")\n\n/**\n * Dev dependencies.\n */\n// Cats &amp; Scala extensions\n// https://mvnrepository.com/artifact/org.typelevel/cats-effect\nlibraryDependencies += \"org.typelevel\" %% \"cats-effect\"        % \"3.5.4\"\n// https://github.com/oleg-py/better-monadic-for (look tags for version)\naddCompilerPlugin(\"com.olegpy\"         %% \"better-monadic-for\" % \"0.3.1\")\n// https://mvnrepository.com/artifact/dev.optics/monocle-core\nlibraryDependencies += \"dev.optics\"    %% \"monocle-core\"       % \"3.2.0\"\n// https://mvnrepository.com/artifact/dev.optics/monocle-macro\nlibraryDependencies += \"dev.optics\"    %% \"monocle-macro\"      % \"3.2.0\"\n\n// Tapir\n// https://mvnrepository.com/artifact/com.softwaremill.sttp.tapir/tapir-http4s-server\nlibraryDependencies += \"com.softwaremill.sttp.tapir\" %% \"tapir-http4s-server\"     % \"1.10.4\"\n// https://mvnrepository.com/artifact/com.softwaremill.sttp.tapir/tapir-json-circe\nlibraryDependencies += \"com.softwaremill.sttp.tapir\" %% \"tapir-json-circe\"        % \"1.10.4\"\n// https://mvnrepository.com/artifact/com.softwaremill.sttp.tapir/tapir-swagger-ui-bundle\nlibraryDependencies += \"com.softwaremill.sttp.tapir\" %% \"tapir-swagger-ui-bundle\" % \"1.10.4\"\n\n// Http4s\n// https://mvnrepository.com/artifact/org.http4s/http4s-ember-server\nlibraryDependencies += \"org.http4s\" %% \"http4s-ember-server\" % \"0.23.26\"\n// https://mvnrepository.com/artifact/org.http4s/http4s-circe\nlibraryDependencies += \"org.http4s\" %% \"http4s-circe\"        % \"0.23.26\"\n// https://mvnrepository.com/artifact/org.http4s/http4s-dsl\nlibraryDependencies += \"org.http4s\" %% \"http4s-dsl\"          % \"0.23.26\"\n// https://mvnrepository.com/artifact/org.http4s/http4s-ember-client\nlibraryDependencies += \"org.http4s\" %% \"http4s-ember-client\" % \"0.23.26\"\n\n// Log4j2\n// https://mvnrepository.com/artifact/org.typelevel/log4cats-core\nlibraryDependencies += \"org.typelevel\" %% \"log4cats-core\" % \"2.6.0\"\n// https://mvnrepository.com/artifact/org.slf4j/slf4j-simple\nlibraryDependencies += \"org.slf4j\"      % \"slf4j-simple\"  % \"2.0.13\"\n\n// Circe\n// https://mvnrepository.com/artifact/io.circe/circe-parser\nlibraryDependencies += \"io.circe\" %% \"circe-parser\"         % \"0.14.6\"\n// https://mvnrepository.com/artifact/io.circe/circe-generic\nlibraryDependencies += \"io.circe\" %% \"circe-generic\"        % \"0.14.6\"\n// https://mvnrepository.com/artifact/io.circe/circe-generic-extras\nlibraryDependencies += \"io.circe\" %% \"circe-generic-extras\" % \"0.14.3\"\n// https://mvnrepository.com/artifact/io.circe/circe-literal\nlibraryDependencies += \"io.circe\" %% \"circe-literal\"        % \"0.14.6\"\n\n// Doobie\n// https://mvnrepository.com/artifact/org.tpolecat/doobie-core\nlibraryDependencies += \"org.tpolecat\" %% \"doobie-core\"     % \"1.0.0-RC5\"\n// https://mvnrepository.com/artifact/org.tpolecat/doobie-postgres\nlibraryDependencies += \"org.tpolecat\" %% \"doobie-postgres\" % \"1.0.0-RC5\"\n// https://mvnrepository.com/artifact/org.tpolecat/doobie-hikari\nlibraryDependencies += \"org.tpolecat\" %% \"doobie-hikari\"   % \"1.0.0-RC5\"\n\n// Flyway\n// https://mvnrepository.com/artifact/org.flywaydb/flyway-core\nlibraryDependencies += \"org.flywaydb\" % \"flyway-core\"                % \"10.11.1\"\n// https://mvnrepository.com/artifact/org.flywaydb/flyway-database-postgresql\nlibraryDependencies += \"org.flywaydb\" % \"flyway-database-postgresql\" % \"10.11.1\" % \"runtime\"\n\n/**\n * Test dependencies.\n */\n// https://mvnrepository.com/artifact/org.scalatest/scalatest\nlibraryDependencies += \"org.scalatest\" %% \"scalatest\"          % \"3.2.18\"  % Test\n// https://mvnrepository.com/artifact/org.mockito/mockito-scala\nlibraryDependencies += \"org.mockito\"   %% \"mockito-scala\"      % \"1.17.31\" % Test\n// https://mvnrepository.com/artifact/org.mockito/mockito-scala-cats\nlibraryDependencies += \"org.mockito\"   %% \"mockito-scala-cats\" % \"1.17.31\" % Test\n</code></pre> <p>To instruct Metals to set up the Scala environment and install the dependencies:</p> <ul> <li><code>CTRL + SHIFT + P</code></li> <li><code>Metals: Import build</code></li> </ul> <p>Then, go to the Metals extension view to confirm it's correctly set up \ud83d\ude0e:</p> Metals extension <p>Warning</p> <p>Metals uses VSCode workspace setup to detect Scala projects. So, it's important to have correctly set up VSCode workspaces as explained previously!</p> <p>We're also going to initialize the backend API and database driver.</p> <ul> <li>First, set up the environement variables \ud83d\udd11.</li> </ul> ./backend/src/main/scala/confs/EnvConf.scala<pre><code>package confs\n\ncase class EnvConf() {\n  private val allEnvVar: Map[String, String] = sys.env\n\n  val devMode: Boolean =\n    allEnvVar.getOrElse(\"TARP_DEV_MODE\",\n                        default = \"true\") == \"true\" // To handle different behaviors in dev and prod environments\n  val port: Int = allEnvVar.getOrElse(\"TARP_PORT\", default = \"8080\").toInt\n\n  val postgresIp: String       = allEnvVar.getOrElse(\"TARP_POSTGRES_IP\", default = \"localhost\")\n  val postgresPort: Int        = allEnvVar.getOrElse(\"TARP_POSTGRES_PORT\", default = \"5432\").toInt\n  val postgresDb: String       = allEnvVar.getOrElse(\"TARP_POSTGRES_DB\", default = \"tarp\")\n  val postgresUser: String     = allEnvVar.getOrElse(\"TARP_POSTGRES_USER\", default = \"tarp\")\n  val postgresPassword: String = allEnvVar.getOrElse(\"TARP_POSTGRES_PASSWORD\", default = \"tarp\")\n  val postgresSchema: String   = allEnvVar.getOrElse(\"TARP_POSTGRES_SCHEMA\", default = \"tarp\")\n}\n\nobject EnvConf { implicit val impl: EnvConf = EnvConf() }\n</code></pre> <ul> <li>Second, integrate \ud83e\udd9b Tapir using http4s under the hood.</li> </ul> ./backend/src/main/scala/confs/ApiConf.scala<pre><code>package confs\n\nimport cats.effect.IO\nimport cats.implicits._\nimport com.comcast.ip4s.IpLiteralSyntax\nimport com.comcast.ip4s.Port\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.server.middleware.{Logger =&gt; LoggerMiddleware}\nimport org.http4s.server.middleware.CORS\nimport org.typelevel.log4cats.Logger\nimport org.typelevel.log4cats.slf4j.Slf4jLogger\nimport org.typelevel.log4cats.syntax.LoggerInterpolator\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\nimport sttp.tapir.swagger.bundle.SwaggerInterpreter\n\ncase class ApiConf()(implicit envConf: EnvConf, logger: Logger[IO] = Slf4jLogger.getLogger) {\n  def setup: IO[Unit] = for {\n    port      &lt;-\n      IO.fromOption(Port.fromInt(envConf.port))(new RuntimeException(s\"Not processable port number ${envConf.port}.\"))\n    corsPolicy = CORS.policy.withAllowOriginHostCi(_ =&gt;\n                   envConf.devMode) // Essential for local development setup with an SPA running on a separate port\n    _         &lt;- EmberServerBuilder\n                   .default[IO]\n                   .withHost(ipv4\"0.0.0.0\")                    // Accept connections from any available network interface\n                   .withPort(port)                             // On a given port\n                   .withHttpApp(corsPolicy(allRts).orNotFound) // Link all routes to the backend server\n                   .build\n                   .use(_ =&gt; IO.never)\n                   .start\n                   .void\n  } yield ()\n\n  private val docsEpt =\n    SwaggerInterpreter().fromEndpoints[IO](List.empty, \"Backend \u2013 TARP Stack \u26fa\", \"1.0\")\n  private val allRts  = {\n    val loggerMiddleware =\n      LoggerMiddleware.httpRoutes(                 // To log incoming requests or outgoing responses from the server\n        logHeaders = true,\n        logBody = true,\n        redactHeadersWhen = _ =&gt; !envConf.devMode, // Display header values exclusively during development mode\n        logAction = Some((msg: String) =&gt; info\"$msg\")\n      )(_)\n    loggerMiddleware(Http4sServerInterpreter[IO]().toRoutes(docsEpt))\n  }\n}\n\nobject ApiConf { implicit val impl: ApiConf = ApiConf() }\n</code></pre> <ul> <li>Lastly, integrate the database driver. Utilize a combination of Doobie for interacting with our database and \u267b\ufe0f Flyway to manage the database schema lifecycle changes.</li> </ul> ./backend/src/main/scala/confs/DbConf.scala<pre><code>package confs\n\nimport cats.effect.IO\nimport cats.effect.Resource\nimport doobie.ConnectionIO\nimport doobie.hikari.HikariTransactor\nimport doobie.implicits._\nimport doobie.util.ExecutionContexts\nimport org.flywaydb.core.Flyway\n\ncase class DbConf()(implicit envConf: EnvConf) {\n  def setup: IO[Unit] = {\n    def raiseUnlessDbUp: IO[Unit] = for {\n      isUp &lt;- run(sql\"SELECT 1;\".query[Long].unique.map(_ == 1L))\n      _    &lt;- IO.raiseUnless(isUp)(new RuntimeException(s\"Postgres ${envConf.postgresDb} database is down.\"))\n    } yield ()\n    def migrate: IO[Unit]         = IO.blocking {\n      Flyway.configure\n        .dataSource(\n          s\"jdbc:postgresql://${envConf.postgresIp}:${envConf.postgresPort}/${envConf.postgresDb}?currentSchema=${envConf.postgresSchema}\",\n          envConf.postgresUser,\n          envConf.postgresPassword\n        )\n        .group(true)\n        .table(\"flyway\")                 // \u26a0\ufe0f \"flyway\" as migration table history (in 'currentSchema' see above)\n        .locations(\"conf/DbConf/flyway\") // \".sql\" files migration resource path\n        .failOnMissingLocations(true)\n        .load\n        .migrate                         // Auto create schema if not exists &amp; Rollback raise exception if failed\n    }\n\n    for {\n      _ &lt;- raiseUnlessDbUp\n      _ &lt;- migrate\n    } yield ()\n  }\n\n  def run[A](sqls: ConnectionIO[A]): IO[A] = transactor.use(sqls.transact[IO])\n\n  private val transactor: Resource[IO, HikariTransactor[IO]] = for {\n    ce &lt;- ExecutionContexts.fixedThreadPool[IO](32)\n    xa &lt;- HikariTransactor.newHikariTransactor[IO](\n            \"org.postgresql.Driver\",\n            s\"jdbc:postgresql://${envConf.postgresIp}:${envConf.postgresPort}/${envConf.postgresDb}?currentSchema=${envConf.postgresSchema}\",\n            envConf.postgresUser,\n            envConf.postgresPassword,\n            ce\n          )\n  } yield xa\n}\n\nobject DbConf { implicit val impl: DbConf = DbConf() }\n</code></pre> <ul> <li>Finally, create a Main class entry point to enable all these components to run.</li> </ul> ./backend/src/main/scala/Main.scala<pre><code>import cats.effect.IO\nimport cats.effect.IOApp\nimport confs.ApiConf\nimport confs.DbConf\n\nobject Main extends IOApp.Simple {\n  override def run: IO[Unit] = DbConf.impl.setup &gt;&gt; ApiConf.impl.setup &gt;&gt; IO.never\n}\n</code></pre> <p>In VSCode, after your code has compiled, you should see a <code>Run</code> \u25b6\ufe0f button appear above your <code>./backend/src/main/scala/Main.scala</code> class. Click \ud83d\udc46 on it, then go to http://localhost:8080/docs/ to see if it worked!</p> SwaggerUI <p>SwaggerUI is accessible for documenting your endpoints for the frontend team. Currently, there are none, but that will change in the second section when we begin building application logic. Let's proceed with setting up the frontend development environment.</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#frontend","title":"Frontend","text":"<p>To begin, we need to install Node.js. It's necessary for running npm commands and installing dependencies. One of the first dependencies we need is a build tool for frontend SPAs. There are quite a few out there, but for me, Vite is the best and fastest when it comes to building SPAs with React. To create a React project using Vite:</p> <ul> <li>Look at your VSCode navigation bar at the top &gt; Click on <code>Terminal</code> &gt; Click on <code>frontend</code> &gt; A terminal for <code>./frontend</code> should appear</li> <li>Then run <code>npm init vite@latest</code> and follow the instructions</li> </ul> <p>For the instructions, make sure to:</p> <ul> <li>Create your project at <code>./frontend</code></li> <li>Use <code>React</code> and not be baited by <code>Preact</code> \ud83d\ude02</li> <li>Select <code>TypeScript + SWC</code> for TypeScript compilation, which includes a Rust-based engine for SPEEEEEEEED! \ud83d\udca8</li> </ul> <p>Files should magically appear in your <code>./frontend</code> VSCode workspace! \ud83c\udf08 Now, let's install the dependencies defined in <code>./frontend/package.json</code>. To do that, run the following command:</p> <pre><code>npm install\n</code></pre> <p>You should see a folder <code>./node_modules</code> appear, where all the dependencies will now reside. From this point, you can run the predefined application that Vite gave you when we initialized the project:</p> <pre><code>npm run dev\n</code></pre> <p>Then, navigate to http://localhost:5173/.</p> \"Vite + React\" page <p>Congratulations, your frontend development environment has been successfully set up \u2705. The development team behind Vite has truly done an amazing job to make it so easy! \ud83d\ude0c</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#building-your-application","title":"\ud83c\udfd7\ufe0f Building Your Application","text":"<p>From the previous section, you should have these three services running locally:</p> <ul> <li>\ud83e\udd9b A Tapir backend with a database driver ready for interaction \u25b6\ufe0f</li> <li>\u269b\ufe0f A React frontend serving the UI \ud83d\udc81\u200d\u2642\ufe0f</li> <li>\ud83d\udc18 A PostgreSQL database up and running \ud83e\udee1</li> </ul> <p>Note</p> <p>It's possible that you don't have exactly the same files in each folder <code>./backend</code>, <code>./frontend</code>, and <code>./devops</code> as presented in the <code>Folder structure</code> figure. This may be due to updates that occurred in the libraries or tools used, or simply because I did not take the time to present all non-mandatory details \ud83e\uddd0. So, if you would like to have exactly the same, don't hesitate to check directly on GitHub!</p> <p>Now, the mission will be to implement some application logic. Nothing overly complicated, but comprehensive. This must involve:</p> <ul> <li>\ud83d\udc18\ud83e\udd1d\ud83e\udd9b Interaction between PostgreSQL and the Tapir backend server</li> <li>\ud83e\udd9b\ud83e\udd1d\u269b\ufe0f Interaction between the Tapir backend server and the React frontend</li> </ul> <p>What about making our last button persistent? \ud83e\udd14 You know, this one:</p> Button clicked 1000 times <p>Currently, I lose all my hard work \ud83d\ude29 if I refresh the page \ud83d\udd03. BUUUUUUUUT, if it's persisted in the database, it will still be there even if I refresh the page \ud83d\ude0f!</p> Frontend, Backend and Database interactions <p>So let's do it! \ud83d\udcaa To be clear, saving a button state in the database makes zero sense \ud83d\ude02!</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#database_1","title":"Database","text":"<p>States are stored in PostgreSQL using tables, so we need to create a table for them. Since there is only one button state, it will consist of a single row with an ID and a count value.</p> <p>Do you recall Flyway from when we worked on the backend? If you check your SQLTools view, you should see a schema named <code>tarp</code>. This schema was created when the Scala backend was launched, specifically when the <code>_.migrate</code> operation completed.</p> <p>The <code>tarp</code> schema is currently managed by Flyway. Our goal here is to instruct Flyway to create our counter table during the <code>_.migrate</code> operation. To do that, simply create the following SQL file:</p> ./backend/src/main/resources/conf/DbConf/flyway/V0__counter_create_table.sql<pre><code>CREATE TABLE IF NOT EXISTS tarp.counter(\n    id BIGSERIAL PRIMARY KEY,\n    count BIGINT NOT NULL\n);\n</code></pre> <p>If you relaunch your backend application, you should see some logs indicating successful migration and a new table in your SQLTools view.</p> Flyway schema migration <p>Info</p> <p>Database schema changes are an inevitable part of continuous application development. As your application evolves, you may need to add or remove columns from existing tables. These changes are closely tied to your backend application logic. One way to address this challenge is to perform database schema migration upon application launch and manage all changes as closely as possible to your backend code. That's why you see the *.sql files defined in our backend folder. This is just one approach; I encourage you to explore other possible solutions.</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#backend_1","title":"Backend","text":"<p>Backend development time! \ud83d\ude0d In order to make our counter application work:</p> <ul> <li>First, let's define the application object.</li> </ul> ./backend/src/main/modules/counter/CounterMod.scala<pre><code>package modules.counter\n\nimport monocle.syntax.all._\n\ncase class CounterMod(id: Long, count: Long) {\n  def addOne: CounterMod = this.focus(_.count).modify(_ + 1)\n}\n</code></pre> <ul> <li>Second, the application logic and persistence.</li> </ul> ./backend/src/main/modules/counter/CounterSvc.scala<pre><code>package modules.counter\n\nimport cats.effect.IO\nimport cats.implicits._\nimport confs.DbConf\nimport doobie.ConnectionIO\n\nfinal case class CounterSvc()(implicit dbConf: DbConf, counterRep: CounterRep) {\n  def getOrCreate: IO[Long] = dbConf.run(counterRep.getOrCreate.map(_.count))\n\n  def addOne: IO[Long] = dbConf.run(for { // Runs atomically within the same database transaction\n    counter       &lt;- counterRep.getOrCreate\n    counterUpdated = counter.addOne\n    _             &lt;- counterRep.update(counterUpdated)\n    count          = counterUpdated.count\n  } yield count)\n}\n\nobject CounterSvc { implicit val impl: CounterSvc = CounterSvc() }\n</code></pre> ./backend/src/main/modules/counter/CounterRep.scala<pre><code>package modules.counter\n\nimport cats.effect.IO\nimport cats.implicits._\nimport confs.DbConf\nimport doobie.ConnectionIO\nimport doobie.implicits._\nimport modules.counter.CounterMod\n\ncase class CounterRep()(implicit dbConf: DbConf) {\n  def getOrCreate: ConnectionIO[CounterMod] = {\n    def createIfNotFound(counterFound: Option[CounterMod]): ConnectionIO[CounterMod] = counterFound match {\n      case Some(counter) =&gt; counter.pure[ConnectionIO]\n      case None          =&gt;\n        sql\"\"\"|INSERT INTO counter (count)\n              |VALUES (0);\"\"\".stripMargin.update.withUniqueGeneratedKeys[Long](\"id\") &gt;&gt;=\n          (id =&gt; sql\"\"\"|SELECT *\n                       |FROM counter\n                       |WHERE id = $id;\"\"\".stripMargin.query[CounterMod].unique)\n    }\n\n    for {\n      counterFound &lt;- sql\"\"\"|SELECT *\n                            |FROM counter\n                            |WHERE id = 1;\"\"\".stripMargin.query[CounterMod].option\n      counter      &lt;- createIfNotFound(counterFound)\n    } yield counter\n  }\n\n  def update(counter: CounterMod): ConnectionIO[Unit] =\n    sql\"\"\"|UPDATE counter\n          |SET count = ${counter.count};\"\"\".stripMargin.update.run.void\n}\n\nobject CounterRep { implicit val impl: CounterRep = CounterRep() }\n</code></pre> <ul> <li>Lastly, the application entry points.</li> </ul> ./backend/src/main/modules/counter/CounterCtrl.scala<pre><code>package modules.counter\n\nimport cats.effect.IO\nimport cats.implicits._\nimport org.http4s.HttpRoutes\nimport sttp.tapir._\nimport sttp.tapir.json.circe._\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\n\nfinal case class CounterCtrl()(implicit counterSvc: CounterSvc) {\n  def endpoints: List[AnyEndpoint] = List(getEpt, addOneEpt)\n  def routes: HttpRoutes[IO]       = getRts &lt;+&gt; addOneRts\n\n  private val getEpt = endpoint.summary(\"Get counter\").get.in(\"api\" / \"counter\").out(jsonBody[Long])\n  private val getRts = Http4sServerInterpreter[IO]().toRoutes(getEpt.serverLogicSuccess(_ =&gt; counterSvc.getOrCreate))\n\n  private val addOneEpt =\n    endpoint.summary(\"Add one to counter\").post.in(\"api\" / \"counter\" / \"add-one\").out(jsonBody[Long])\n  private val addOneRts = Http4sServerInterpreter[IO]().toRoutes(addOneEpt.serverLogicSuccess(_ =&gt; counterSvc.addOne))\n}\n\nobject CounterCtrl { implicit val impl: CounterCtrl = CounterCtrl() }\n</code></pre> <p>Then, make all these components accessible for the frontend. Let's wrap up by exposing our new endpoints. Let's also move our logger middleware only to application entry points.</p> ./backend/src/main/scala/confs/ApiConf.scala<pre><code>package confs\n\nimport cats.effect.IO\nimport cats.implicits._\nimport com.comcast.ip4s.IpLiteralSyntax\nimport com.comcast.ip4s.Port\nimport modules.counter.CounterCtrl\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.server.middleware.{Logger =&gt; LoggerMiddleware}\nimport org.http4s.server.middleware.CORS\nimport org.typelevel.log4cats.Logger\nimport org.typelevel.log4cats.slf4j.Slf4jLogger\nimport org.typelevel.log4cats.syntax.LoggerInterpolator\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\nimport sttp.tapir.swagger.bundle.SwaggerInterpreter\n\nfinal case class ApiConf()(implicit\n    envConf: EnvConf,\n    counterCtrl: CounterCtrl,\n    logger: Logger[IO] = Slf4jLogger.getLogger) {\n  def setup: IO[Unit] = for {\n    port      &lt;-\n      IO.fromOption(Port.fromInt(envConf.port))(new RuntimeException(s\"Not processable port number ${envConf.port}.\"))\n    corsPolicy = CORS.policy.withAllowOriginHostCi(_ =&gt;\n                   envConf.devMode) // Essential for local development setup with an SPA running on a separate port\n    _         &lt;- EmberServerBuilder\n                   .default[IO]\n                   .withHost(ipv4\"0.0.0.0\")                    // Accept connections from any available network interface\n                   .withPort(port)                             // On a given port\n                   .withHttpApp(corsPolicy(allRts).orNotFound) // Link all routes to the backend server\n                   .build\n                   .use(_ =&gt; IO.never)\n                   .start\n                   .void\n  } yield ()\n\n  private val docsEpt =\n    SwaggerInterpreter().fromEndpoints[IO](counterCtrl.endpoints, // Here's where the new endpoint definition is added!\n                                           \"Backend \u2013 TARP Stack \u26fa\",\n                                           \"1.0\")\n  private val allRts  = {\n    val loggerMiddleware =\n      LoggerMiddleware.httpRoutes(                 // To log incoming requests or outgoing responses from the server\n        logHeaders = true,\n        logBody = true,\n        redactHeadersWhen = _ =&gt; !envConf.devMode, // Display header values exclusively during development mode\n        logAction = Some((msg: String) =&gt; info\"$msg\")\n      )(_)\n    Http4sServerInterpreter[IO]().toRoutes(docsEpt) &lt;+&gt;\n      loggerMiddleware(counterCtrl.routes) // Here's where the new endpoint logic is added!\n  }\n}\n\nobject ApiConf { implicit val impl: ApiConf = ApiConf() }\n</code></pre> <p>Hehe, it's done! Our new two endpoints are available at http://localhost:8080/docs/ \ud83e\udd29!</p> Endpoint interaction via SwaggerUI <p>As you can see, it's possible to interact with your endpoints directly via SwaggerUI.</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#frontend_1","title":"Frontend","text":"<p>The objective is to interact with the backend. In order to achieve that, let's first add some dependencies:</p> <ul> <li>\ud83d\udd04 React Query: To manage asynchronous data fetching, caching, and updating with our backend.</li> <li>\ud83d\udedc Axios: For making HTTP requests.</li> <li>\ud83d\udee0\ufe0f OpenAPI TypeScript: To automatically generate TypeScript schemas from OpenAPI specifications.</li> </ul> ./frontend/package.json<pre><code>{\n  \"name\": \"frontend\",\n  \"private\": true,\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"tsc &amp;&amp; vite build\",\n    \"lint\": \"eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0\",\n    \"preview\": \"vite preview\"\n  },\n  \"dependencies\": {\n    \"@tanstack/react-query\": \"^5.31.0\",\n    \"axios\": \"^1.6.8\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\"\n  },\n  \"devDependencies\": {\n    \"@types/react\": \"^18.2.79\",\n    \"@types/react-dom\": \"^18.2.25\",\n    \"@typescript-eslint/eslint-plugin\": \"^7.7.0\",\n    \"@typescript-eslint/parser\": \"^7.7.0\",\n    \"@vitejs/plugin-react-swc\": \"^3.6.0\",\n    \"eslint\": \"^8.57.0\",\n    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n    \"eslint-plugin-react-refresh\": \"^0.4.6\",\n    \"openapi-typescript\": \"^6.7.5\",\n    \"typescript\": \"^5.4.5\",\n    \"vite\": \"^5.2.10\"\n  }\n}\n</code></pre> <p>To install these new dependencies in your <code>./frontend/node_modules/</code> folder, simply run <code>npm update --save</code>. Then, we should define our Axios service, which will interact with the backend.</p> ./frontend/src/services/backend/index.ts<pre><code>import axios from \"axios\";\n\nexport function useBackend() {\n  const backend = axios.create({\n    baseURL: import.meta.env.DEV ? \"http://localhost:8080\" : undefined, // During development, the backend is hosted at 'localhost:8080'\n  });\n  return { backend };\n}\n</code></pre> <p>Now, for something truly magical: automatically generating backend schemas using OpenAPI TypeScript.</p> <pre><code>npx openapi-typescript http://localhost:8080/docs/docs.yaml --output ./src/services/backend/endpoints.d.ts\n</code></pre> <p>A new file should have popped up at <code>./frontend/src/services/backend/endpoints.d.ts</code>, which is an exact replica of our SwaggerUI but in TypeScript. In our case, it might not be as impressive, but just imagine in an actual enterprise backend which may contain 20 to 100 endpoints. How many days of work that would take for a frontend team to properly manage the schemas \ud83d\ude23? And seriously, who wants to do a job of just copying SwaggerUI schemas into TypeScript interfaces? Not you? Not me \ud83d\udca9! This library is truly the holy grail \ud83d\ude4f! Don't forget to leave a star on their GitHub repository!</p> <p>Alright, now that the backend service is set up, let's dive into some coding, or should I say, let's just re-adapt the code that is already generated by Vite \ud83d\udc37:</p> <ul> <li>First, install \ud83d\udee3\ufe0f React Router: It manages the frontend routes displayed in the user's browser.</li> </ul> ./frontend/package.json<pre><code>{\n  \"name\": \"frontend\",\n  \"private\": true,\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"tsc &amp;&amp; vite build\",\n    \"lint\": \"eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0\",\n    \"preview\": \"vite preview\"\n  },\n  \"dependencies\": {\n    \"@tanstack/react-query\": \"^5.31.0\",\n    \"axios\": \"^1.6.8\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\",\n    \"react-router-dom\": \"^6.22.3\"\n  },\n  \"devDependencies\": {\n    \"@types/react\": \"^18.2.79\",\n    \"@types/react-dom\": \"^18.2.25\",\n    \"@typescript-eslint/eslint-plugin\": \"^7.7.0\",\n    \"@typescript-eslint/parser\": \"^7.7.0\",\n    \"@vitejs/plugin-react-swc\": \"^3.6.0\",\n    \"eslint\": \"^8.57.0\",\n    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n    \"eslint-plugin-react-refresh\": \"^0.4.6\",\n    \"openapi-typescript\": \"^6.7.5\",\n    \"typescript\": \"^5.4.5\",\n    \"vite\": \"^5.2.10\"\n  }\n}\n</code></pre> <ul> <li>Second, let's re-adapt the existing page as a component. I prefer to separate component logic, styles, and template.</li> </ul> ./frontend/src/modules/counter/CounterPg/useCounter.ts<pre><code>import { useBackend } from \"@/services/backend\";\nimport { paths } from \"@/services/backend/endpoints\";\nimport { useMutation, useQuery, useQueryClient } from \"@tanstack/react-query\";\n\nexport function useCounter() {\n  const { backend } = useBackend();\n  const queryClient = useQueryClient();\n  const { data: count, isLoading } = useQuery({\n    queryKey: [\"/api/counter\"],\n    queryFn: () =&gt;\n      backend\n        .get&lt;\n          paths[\"/api/counter\"][\"get\"][\"responses\"][\"200\"][\"content\"][\"application/json\"] // Type safety using the schema generated from OpenAPI TypeScript\n        &gt;(\"/api/counter\")\n        .then((_) =&gt; _.data),\n  });\n  const { mutate: addOne, isPending } = useMutation({\n    mutationFn: () =&gt;\n      backend\n        .post&lt;\n          paths[\"/api/counter/add-one\"][\"post\"][\"responses\"][\"200\"][\"content\"][\"application/json\"] // Type safety using the schema generated from OpenAPI TypeScript\n        &gt;(\"/api/counter/add-one\")\n        .then((_) =&gt; _.data),\n    onSuccess: (data) =&gt; queryClient.setQueryData([\"/api/counter\"], data), // It refreshes the cached with the new received counter\n  });\n\n  return {\n    count,\n    addOne: () =&gt; addOne(),\n    isProgressing: isLoading || isPending,\n  };\n}\n</code></pre> ./frontend/src/modules/counter/CounterPg/Counter.css<pre><code>#root {\n  max-width: 1280px;\n  margin: 0 auto;\n  padding: 2rem;\n  text-align: center;\n}\n\n.logo {\n  height: 6em;\n  padding: 1.5em;\n  will-change: filter;\n  transition: filter 300ms;\n}\n.logo:hover {\n  filter: drop-shadow(0 0 2em #646cffaa);\n}\n.logo.react:hover {\n  filter: drop-shadow(0 0 2em #61dafbaa);\n}\n\n@keyframes logo-spin {\n  from {\n    transform: rotate(0deg);\n  }\n  to {\n    transform: rotate(360deg);\n  }\n}\n\n@media (prefers-reduced-motion: no-preference) {\n  a:nth-of-type(2) .logo {\n    animation: logo-spin infinite 20s linear;\n  }\n}\n\n.card {\n  padding: 2em;\n}\n\n.read-the-docs {\n  color: #888;\n}\n</code></pre> ./frontend/src/modules/counter/CounterPg/index.tsx<pre><code>import viteLogo from \"/vite.svg\";\nimport \"@/modules/counter/CounterPg/Counter.css\";\nimport reactLogo from \"@/assets/react.svg\";\nimport { useCounter } from \"@/modules/counter/CounterPg/useCounter\";\n\nexport function CounterPg() {\n  const { count, addOne, isProgressing } = useCounter();\n\n  return (\n    &lt;&gt;\n      &lt;div&gt;\n        &lt;a href=\"https://vitejs.dev\" target=\"_blank\"&gt;\n          &lt;img src={viteLogo} className=\"logo\" alt=\"Vite logo\" /&gt;\n        &lt;/a&gt;\n        &lt;a href=\"https://react.dev\" target=\"_blank\"&gt;\n          &lt;img src={reactLogo} className=\"logo react\" alt=\"React logo\" /&gt;\n        &lt;/a&gt;\n      &lt;/div&gt;\n      &lt;h1&gt;Frontend \u2013 TARP Stack \u26fa&lt;/h1&gt;\n      &lt;div className=\"card\"&gt;\n        &lt;button onClick={() =&gt; addOne()} disabled={isProgressing}&gt;\n          count is {count}\n        &lt;/button&gt;\n        &lt;p&gt;\n          Edit &lt;code&gt;src/App.tsx&lt;/code&gt; and save to test HMR\n        &lt;/p&gt;\n      &lt;/div&gt;\n      &lt;p className=\"read-the-docs\"&gt;\n        Click on the Vite and React logos to learn more\n      &lt;/p&gt;\n    &lt;/&gt;\n  );\n}\n</code></pre> <p>Lastly, let's wrap things up by exposing our new component at <code>/</code>. Also, let's take the time to set up a redirection to <code>/</code> in case of a frontend route not found.</p> ./frontend/src/App.tsx<pre><code>import { QueryClientProvider, QueryClient } from \"@tanstack/react-query\";\nimport { Routes, Route, BrowserRouter, Navigate } from \"react-router-dom\";\nimport { CounterPg } from \"@/modules/counter/CounterPg\";\n\nfunction App() {\n  return (\n    &lt;main&gt;\n      &lt;BrowserRouter&gt;\n        &lt;QueryClientProvider client={new QueryClient()}&gt;\n          &lt;Routes&gt;\n            &lt;Route path=\"/\" element={&lt;CounterPg /&gt;} /&gt;\n            {&lt;Route path=\"*\" element={&lt;Navigate to=\"/\" /&gt;} /&gt;}\n          &lt;/Routes&gt;\n        &lt;/QueryClientProvider&gt;\n      &lt;/BrowserRouter&gt;\n    &lt;/main&gt;\n  );\n}\n\nexport default App;\n</code></pre> <p>If you navigate to http://localhost:5173/ (ensuring your database and backend are running), you should be able to enjoy your persisted click count! \ud83c\udf89</p> Persisted count <p>Are you feeling the power of the FullStack developer rising within you right now \ud83d\ude24? You should, because you've just created an application from the database to the frontend, going through the backend \ud83d\ude09! Even though it's just a simple one, I'm sure you can manage from here \ud83d\udc4d. The hard part is always the starting point.</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#wrapping-up-for-production","title":"\ud83c\udf81 Wrapping Up For Production","text":"<p>From the previous section, you should have these three services running and interacting locally:</p> <ul> <li>\ud83e\udd9b A Tapir backend with 2 exposed endpoints and handling database requests on endpoint invocation \u25b6\ufe0f</li> <li>\u269b\ufe0f A React frontend consuming the 2 exposed endpoints \ud83d\udc81\u200d\u2642\ufe0f</li> <li>\ud83d\udc18 A PostgreSQL database with one table storing the click count \ud83e\udee1</li> </ul> <p>Now, the mission will be to prepare for production using Docker. More precisely, build a Docker image for our DevOps team to deploy. The Docker container has to be able to serve our frontend and backend simultaneously. However, currently our Tapir backend and React frontend are running on two different servers. So, I'm going to show you how you can serve your React SPA using Tapir.</p> SPA Frontend = Static files for the browser to interpret"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#serving-react-via-tapir","title":"Serving React via Tapir","text":"<p>To serve your React SPA via Tapir, there are only 2 steps:</p> <ul> <li>Build your React SPA into a distributable \ud83c\udf81 package.</li> </ul> <pre><code>npm run build\n</code></pre> npm run build <ul> <li>Serve this distributable via a Tapir endpoint.</li> </ul> ./backend/src/main/scala/confs/ApiConf.scala<pre><code>package confs\n\nimport cats.effect.IO\nimport cats.implicits._\nimport com.comcast.ip4s.IpLiteralSyntax\nimport com.comcast.ip4s.Port\nimport modules.counter.CounterCtrl\nimport org.http4s.ember.server.EmberServerBuilder\nimport org.http4s.server.middleware.{Logger =&gt; LoggerMiddleware}\nimport org.http4s.server.middleware.CORS\nimport org.typelevel.log4cats.Logger\nimport org.typelevel.log4cats.slf4j.Slf4jLogger\nimport org.typelevel.log4cats.syntax.LoggerInterpolator\nimport sttp.tapir.emptyInput\nimport sttp.tapir.files.FilesOptions\nimport sttp.tapir.files.staticFilesGetServerEndpoint\nimport sttp.tapir.server.http4s.Http4sServerInterpreter\nimport sttp.tapir.swagger.bundle.SwaggerInterpreter\n\nfinal case class ApiConf()(implicit\n    envConf: EnvConf,\n    counterCtrl: CounterCtrl,\n    logger: Logger[IO] = Slf4jLogger.getLogger) {\n  def setup: IO[Unit] = for {\n    port      &lt;-\n      IO.fromOption(Port.fromInt(envConf.port))(new RuntimeException(s\"Not processable port number ${envConf.port}.\"))\n    corsPolicy = CORS.policy.withAllowOriginHostCi(_ =&gt;\n                   envConf.devMode) // Essential for local development setup with an SPA running on a separate port\n    _         &lt;- EmberServerBuilder\n                   .default[IO]\n                   .withHost(ipv4\"0.0.0.0\")                    // Accept connections from any available network interface\n                   .withPort(port)                             // On a given port\n                   .withHttpApp(corsPolicy(allRts).orNotFound) // Link all routes to the backend server\n                   .build\n                   .use(_ =&gt; IO.never)\n                   .start\n                   .void\n  } yield ()\n\n  private val frontendServerLogic =\n    staticFilesGetServerEndpoint[IO](emptyInput)(\n      \"../frontend/dist\",                                  // Serve frontend static files from '../frontend/dist' at '/'\n      FilesOptions.default.defaultFile(List(\"index.html\")) // Serve '../frontend/dist/index.html' if not found\n    )\n  private val frontendEpt         =\n    frontendServerLogic.endpoint.summary(\"Frontend served from '../frontend/dist' on the file system\")\n  private val frontendRts         = Http4sServerInterpreter[IO]().toRoutes(frontendServerLogic)\n\n  private val docsEpt =\n    SwaggerInterpreter().fromEndpoints[IO](\n      counterCtrl.endpoints :+ // Here's where the new endpoint definition is added!\n        frontendEpt,\n      \"Backend \u2013 TARP Stack \u26fa\",\n      \"1.0\")\n  private val allRts  = {\n    val loggerMiddleware =\n      LoggerMiddleware.httpRoutes(                 // To log incoming requests or outgoing responses from the server\n        logHeaders = true,\n        logBody = true,\n        redactHeadersWhen = _ =&gt; !envConf.devMode, // Display header values exclusively during development mode\n        logAction = Some((msg: String) =&gt; info\"$msg\")\n      )(_)\n    Http4sServerInterpreter[IO]().toRoutes(docsEpt) &lt;+&gt;\n      loggerMiddleware(counterCtrl.routes) &lt;+&gt; // Here's where the new endpoint logic is added!\n      frontendRts // Endpoints are resolved by order, so it's crucial to place frontend logic last! Otherwise, it will always serve 'index.html'!\n  }\n}\n\nobject ApiConf { implicit val impl: ApiConf = ApiConf() }\n</code></pre> <p>Then we're done! \ud83d\udc4f Already?! Yep, Tapir's <code>staticFilesGetServerEndpoint</code> method did everything for us \ud83d\ude0c. If you go to http://localhost:8080 while running only your backend, you should see your frontend being served.</p> Serving React via Tapir <p>Warning</p> <p>Two crucial rules to adhere to when serving any frontend SPA application from your custom backend: Firstly, it's imperative to avoid using <code>/api</code> or any other subpath used by the backend for routing within the frontend. Secondly, ensure that the backend serves <code>index.html</code> in case of a non-existent endpoint, giving back control of the browser's routing to the frontend.</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#optimized-docker-build","title":"Optimized Docker Build","text":"<p>Final part! Now onto building the Docker image. It's crucial to aim for a swift Docker build and the most lightweight Docker image possible. To pursue these goals as effectively as we can \ud83c\udf08, we'll leverage the Docker build cache and employ Multi-stage builds.</p> <ul> <li>Regarding the \ud83d\uddc4\ufe0f Docker build cache: It's crucial to prioritize your less frequently altered and heaviest image layers first, typically those involving dependency installations. By doing so, the Docker image build process will attempt to utilize cached image layers instead of rebuilding them.</li> <li>As for the \ud83c\udfd7\ufe0f Multi-stage builds: Certain image layers are temporary and exist solely for building the application, rather than for running it. Multi-stage builds offer a means to eliminate these unnecessary image layers from our final Docker image.</li> </ul> <p>Here are the <code>.dockerignore</code> and <code>Dockerfile</code> files.</p> ./devops/prod-build/Dockerfile.dockerignore<pre><code># Backend\n/backend/**/.bloop/\n/backend/.metals/\n/backend/.vscode/\n/backend/**/target/\n/backend/**/metals.sbt\n/backend/src/test/\n/backend/.gitignore\n/backend/.scalafmt.conf\n/backend/README.md\n\n# Frontend\n/frontend/.vscode\n/frontend/dist/\n/frontend/node_modules/\n/frontend/.eslintrc.cjs\n/frontend/.gitignore\n/frontend/README.md\n</code></pre> ./devops/prod-build/Dockerfile<pre><code>FROM eclipse-temurin:17.0.10_7-jre AS backend_builder\nWORKDIR /build/backend\n# Start SBT install via Coursier: https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Linux.html\nRUN curl -fLo coursier https://github.com/coursier/launchers/raw/master/coursier \\\n    &amp;&amp; chmod +x coursier \\\n    &amp;&amp; ./coursier setup -y\n# End\nCOPY backend/project /build/backend/project\nCOPY backend/build.sbt /build/backend/\nRUN /root/.local/share/coursier/bin/sbt update\nCOPY backend /build/backend\nRUN /root/.local/share/coursier/bin/sbt package \\\n    &amp;&amp; mkdir dist \\\n    &amp;&amp; /root/.local/share/coursier/bin/sbt \"export Runtime / fullClasspathAsJars\" | tail -n 1 | tr \":\" \"\\n\" | xargs -I '{}' mv '{}' /build/backend/dist\n\nFROM node:20.12-slim AS frontend_builder\nWORKDIR /build/frontend\nCOPY frontend/package.json frontend/package-lock.json /build/frontend/\nRUN npm install\nCOPY frontend /build/frontend\nRUN npm run build\n\nFROM eclipse-temurin:17.0.10_7-jre\nWORKDIR /app/backend\nCOPY --from=backend_builder /build/backend/dist /app/backend/dist\nCOPY --from=frontend_builder /build/frontend/dist /app/frontend/dist\nENV TARP_DEV_MODE=\"false\"\nEXPOSE 8080\nENTRYPOINT [ \"java\", \"-cp\", \"/app/backend/dist/*\", \"Main\" ]\n</code></pre> <p>Tip</p> <p>To build your final Docker image, ensure to utilize docker buildx to render your image compatible with two processor architectures - amd64 and arm64. Why, you may ask? Well, according to the Stack Oveflow Survey 2023, the most commonly used operating systems among developers are Windows, MacOS, and Ubuntu, which frequently implies the presence of amd64 and arm64 processor architectures.</p> <p>To build and push your multi-architecture compatible Docker image to the registry, execute the following two commands:</p> <pre><code>docker buildx create --use --name multi-platform-builder\n</code></pre> <pre><code>docker buildx build -t my_registry_to_replace/tarp:0.0.0 -t my_registry_to_replace/tarp:latest --platform linux/amd64,linux/arm64 --push -f \"prod-build/Dockerfile\" ..\n</code></pre> <p>Here are two commands I often use for implementing and debugging the Dockerfile:</p> <pre><code>docker build -t my_registry_to_replace/tarp:local -f \"prod-build/Dockerfile\" ..\n</code></pre> <pre><code>docker run -it --entrypoint /bin/sh my_registry_to_replace/tarp:local\n</code></pre> <p>After the execution is complete, if you navigate to your Docker registry, you should be able to see your images.</p> Deployable Docker images <p>Mission accomplished! \ud83c\udf8a</p>"},{"location":"2024/04/18/introducing-tarp-stack---tapir-react-and-postgresql/#full-stack-developer-reached","title":"\ud83c\udf1f Full-Stack Developer Reached!","text":"<p>If you've reached this conclusion, big congratulations! \ud83c\udf89 You are now a fellow FullStack developer, master of the TARP stack \ud83d\ude0e. Know this, the fun is only just beginning because you now have the superpower to build your own application \ud83d\ude2f! If you do, please don't hesitate to share it. I am extremely curious to know!</p> <p>I write monthly on the LovinData Blog and on Medium, and like to give back the knowledge I've learned. So don't hesitate to reach out; I'm always available to chat about nerdy stuff \ud83e\udd17! Here are my socials: LinkedIn, Twitter and Reddit. Otherwise, let's learn together in the next story \ud83e\udee1! Bye \u2764\ufe0f.</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/","title":"Mastering Spark on K8s \ud83d\udd25 and Why I Dumped \ud83d\udc94 Kubeflow Spark Operator (Formerly Google's Spark Operator)!","text":"<p>Heyoooo Spark \u26a1 developers! My product manager several months ago asked me one question: \"Is it possible to run Spark applications without K8s \ud83d\udc33 cluster-level access?\" At the time, I only knew the Kubeflow \ud83d\udd27 Spark Operator well and was using it for deploying all my Spark applications. For those who know, you must have K8s cluster-level access to use the Kubeflow Spark Operator. The reasons are because it installs CRDs and ClusterRole. So I told him \"no\" with these reasons, and on his side, he tried his best to convince the prospect with the constraint in mind. At the enterprise level, they usually have a multi-tenant K8s cluster segregated by company/department, project, and environment (dev, uat, pre-prod, or prod) using Namespaces. This way, they make the most of the computing resources allocated. Plus, if one project does not meet the expectation or the contract ends, hop hop <code>kubectl delete &lt;compordept&gt;-&lt;project&gt;-&lt;env&gt;</code> and it's like the project has never existed. I am currently writing to tell my product manager, \"Yes, it's possible to run Spark applications without K8s cluster-level access.\"! Here is how! \ud83d\ude80</p> <p></p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#k8s-spark-and-kubeflow","title":"\ud83e\udd14 K8s, Spark and Kubeflow?","text":"<p>What are K8s, Spark, and Kubeflow Spark Operator? Quickly:</p> <ul> <li>\ud83d\udc33 K8s: Kubernetes (K8s) is an open-source container orchestration platform designed to automate the deployment, scaling, and management of containerized applications, often used to orchestrate complex frameworks like Apache Spark for efficient data processing at scale.</li> <li>\u26a1 Spark: Apache Spark is an open-source distributed computing framework that enables fast data processing and analytics, widely recognized and supported by the company Databricks in big data environments.</li> <li>\ud83d\udd27 Kubeflow Spark Operator: The Kubeflow Spark Operator facilitates seamless integration of Apache Spark with Kubernetes. Originally developed by Google Cloud Platform, it has recently been donated to the Kubeflow community.</li> </ul> <p>At the end of this guide, you should also be able to launch a Spark application on a Kubernetes cluster and understand when to use basic Spark CLI or the Kubeflow Spark Operator. Let's get started!</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#local-k8s-for-development","title":"\ud83d\udc68\u200d\ud83d\udcbb Local K8s for development","text":"<p>In this part, we are going to install a one-node K8s locally and simulate an enterprise K8s segregated by namespaces.</p> Segregate by namespaces <p>The goal is to set up the prerequisites for when we are going to launch Spark applications.</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#installation","title":"Installation","text":"<p>In the past, there was minikube for local K8s development. But now, Docker Desktop has integrated Kubernetes directly \ud83e\udd29! So let's install Docker Desktop.</p> Downloading Docker Dekstop <p>Then you just need to launch the executable and follow the instructions. Once you have successfully installed Docker Desktop, to have your single-node K8s cluster:</p> <ul> <li>Go to <code>Settings (top-right) &gt; Kubernetes</code></li> <li>Check the box <code>Enable Kubernetes</code></li> <li>Click on <code>Apply &amp; restart</code> &gt; Click on <code>Install</code></li> <li>It should start pulling necessary Docker images</li> </ul> <p>After a while, you should see the screen with the little <code>Kubernetes running</code> on the bottom left. You can validate your installation by opening a terminal, then the following command.</p> <pre><code>kubectl get nodes\n</code></pre> <p>It should return an output similar to this:</p> <pre><code>NAME             STATUS   ROLES           AGE     VERSION\ndocker-desktop   Ready    control-plane   6d23h   v1.29.2\n</code></pre> <p>Nice! \ud83c\udf89 You've successfully installed Kubernetes.</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#isolated-namespace","title":"Isolated namespace","text":"<p>The goal now is to set up isolated namespaces to simulate an enterprise multi-tenant Kubernetes cluster. To set up an isolated namespace, here are the essentials:</p> <ul> <li>\ud83d\udce6\ud83c\udf10 A namespace under quota: Kubernetes resources are not unlimited. The relevant resources to limit are CPU, RAM, ephemeral storage, number of pods, and other resources as necessary.</li> <li>\ud83d\udc69\u200d\ud83d\udcbb\ud83c\udf10 An admin namespace role: Users must be restricted to a namespace, but within this namespace, Kubernetes admins should provide all necessary access for them to operate autonomously. However, they should not have permission to create, update, or delete quotas and roles.</li> <li>\ud83d\udd04\ud83c\udf10 Expirable access: Projects do not last indefinitely, so access should not be permanent either.</li> </ul> <p>Let's get to work!</p> <p>First, the namespace under quota:</p> <pre><code>kubectl create ns compordept-project-env # Create the namespace.\nkubectl config set-context docker-desktop --namespace=compordept-project-env # Switch to the namespace.\necho '\n---\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: namespace-quota\nspec:\n  hard:\n    limits.memory: \"1939Mi\"\n    requests.cpu: \"3\"\n    requests.memory: \"1939Mi\"\n    persistentvolumeclaims: \"9\"\n    requests.ephemeral-storage: \"227Gi\"\n    limits.ephemeral-storage: \"227Gi\"\n    pods: \"27\"\n    services: \"9\"\n' | kubectl apply -f -\n</code></pre> <p>Second, the admin namespace role:</p> <pre><code>echo '\n# This script lists all namespaced resources and sub-resources except \"resourcequotas\" and \"role\". The goal is to have them in a final YAML array which we can pipe to the kubectl command.\n\nimport subprocess, json\nkubectl_get_raw_as_dict = lambda path: json.loads(subprocess.check_output(f\"kubectl get --raw {path}\", shell=True, text=True, stderr=subprocess.PIPE))\nprint_as_yaml_array = lambda list_of_strings: print(json.dumps(list_of_strings))\n\npaths = kubectl_get_raw_as_dict(\"/\")[\"paths\"]\noutput_resources = []\nfor path in paths:\n  try:\n    resources = kubectl_get_raw_as_dict(path)[\"resources\"]\n    resources = [resource[\"name\"] for resource in resources if resource[\"namespaced\"] == True and (resource[\"name\"] not in [\"resourcequotas\", \"roles\"])]\n    output_resources.extend(resources)\n  except:\n    pass\nprint_as_yaml_array(output_resources) # Print to the console for piping.\n' | python3 | xargs -I {} echo '\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: namespace-admin\nrules:\n  - apiGroups: [\"*\"]\n    resources: {} # Piped here!\n    verbs: [\"*\"]\n  - apiGroups: [\"*\"]\n    resources: [\"resourcequotas\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"*\"]\n    resources: [\"roles\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: namespace-admin\nsubjects:\n  - kind: Group\n    name: namespace-admin\n    apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: namespace-admin\n  apiGroup: rbac.authorization.k8s.io\n' | kubectl apply -f -\n</code></pre> <p>Info</p> <p>The command <code>kubectl api-resources --namespaced=true</code> unfortunately does not list the sub-resources for those who thought about it. This means if this command is used as a base, the sub-resource <code>pods/log</code> won't be set for our <code>namespace-admin</code>.</p> <p>Third, the expirable access:</p> <pre><code># Generate CSR (on user side)\nopenssl genrsa -out compordept-project-env-admin.key 2048\nopenssl req -new -key compordept-project-env-admin.key -out compordept-project-env-admin.csr -subj \"/CN=namespace-admin/O=namespace-admin\"\n\n# Validate CSR &amp; Generate CRT (on K8S admin side)\ncat compordept-project-env-admin.csr | base64 | tr -d \"\\n\" | xargs -I {} echo '\n---\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: compordept-project-env-admin\nspec:\n  request: {}\n  signerName: kubernetes.io/kube-apiserver-client\n  expirationSeconds: 7898368 # 3 months\n  usages:\n    - client auth\n' | kubectl apply -f -\nkubectl certificate approve compordept-project-env-admin\nkubectl get csr compordept-project-env-admin -o jsonpath='{.status.certificate}'| base64 -d &gt; compordept-project-env-admin.crt # Certificate to give to your user\n\n# Set CRT and use created user (on user side)\nkubectl config set-credentials compordept-project-env-admin --client-key=compordept-project-env-admin.key --client-certificate=compordept-project-env-admin.crt --embed-certs=true\nkubectl config set-context docker-desktop --user=compordept-project-env-admin\nrm compordept-project-env-admin.* # Purge certificates from disk\n</code></pre> <p>Warning</p> <p>As you can see by the comments, creating the <code>namespace-admin</code> user involves the K8s admin and the requester in a perfect world. But, for simplicity, the K8s admin can also just run all the commands and provide the final <code>compordept-project-env-admin.key</code> and <code>compordept-project-env-admin.crt</code>.</p> <p>If you've reached this point, you should be set in the namespace <code>compordept-project-env-admin</code> as the user <code>namespace-admin</code>! The following command should list you the <code>namespace-admin</code> role.</p> <pre><code>kubectl get role\n</code></pre> <p>This role can be viewed using:</p> <pre><code>kubectl describe role namespace-admin\n</code></pre> <pre><code>Name:         namespace-admin\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nPolicyRule:\n  Resources                          Non-Resource URLs  Resource Names  Verbs\n  ---------                          -----------------  --------------  -----\n  bindings.*                         []                 []              [*]\n  configmaps.*                       []                 []              [*]\n  controllerrevisions.*              []                 []              [*]\n  cronjobs.*/status                  []                 []              [*]\n  cronjobs.*                         []                 []              [*]\n  csistoragecapacities.*             []                 []              [*]\n  daemonsets.*/status                []                 []              [*]\n  daemonsets.*                       []                 []              [*]\n  deployments.*/scale                []                 []              [*]\n  deployments.*/status               []                 []              [*]\n  deployments.*                      []                 []              [*]\n  endpoints.*                        []                 []              [*]\n  endpointslices.*                   []                 []              [*]\n  events.*                           []                 []              [*]\n  horizontalpodautoscalers.*/status  []                 []              [*]\n  horizontalpodautoscalers.*         []                 []              [*]\n  ingresses.*/status                 []                 []              [*]\n  ingresses.*                        []                 []              [*]\n  jobs.*/status                      []                 []              [*]\n  jobs.*                             []                 []              [*]\n  leases.*                           []                 []              [*]\n  limitranges.*                      []                 []              [*]\n  localsubjectaccessreviews.*        []                 []              [*]\n  networkpolicies.*                  []                 []              [*]\n  persistentvolumeclaims.*/status    []                 []              [*]\n  persistentvolumeclaims.*           []                 []              [*]\n  poddisruptionbudgets.*/status      []                 []              [*]\n  poddisruptionbudgets.*             []                 []              [*]\n  pods.*/attach                      []                 []              [*]\n  pods.*/binding                     []                 []              [*]\n  pods.*/ephemeralcontainers         []                 []              [*]\n  pods.*/eviction                    []                 []              [*]\n  pods.*/exec                        []                 []              [*]\n  pods.*/log                         []                 []              [*]\n  pods.*/portforward                 []                 []              [*]\n  pods.*/proxy                       []                 []              [*]\n  pods.*/status                      []                 []              [*]\n  pods.*                             []                 []              [*]\n  podtemplates.*                     []                 []              [*]\n  replicasets.*/scale                []                 []              [*]\n  replicasets.*/status               []                 []              [*]\n  replicasets.*                      []                 []              [*]\n  replicationcontrollers.*/scale     []                 []              [*]\n  replicationcontrollers.*/status    []                 []              [*]\n  replicationcontrollers.*           []                 []              [*]\n  resourcequotas.*/status            []                 []              [*]\n  rolebindings.*                     []                 []              [*]\n  secrets.*                          []                 []              [*]\n  serviceaccounts.*/token            []                 []              [*]\n  serviceaccounts.*                  []                 []              [*]\n  services.*/proxy                   []                 []              [*]\n  services.*/status                  []                 []              [*]\n  services.*                         []                 []              [*]\n  statefulsets.*/scale               []                 []              [*]\n  statefulsets.*/status              []                 []              [*]\n  statefulsets.*                     []                 []              [*]\n  resourcequotas.*                   []                 []              [get list watch]\n  roles.*                            []                 []              [get list watch]\n</code></pre> <p>This command should give you a \"forbidden access\" error.</p> <pre><code>kubectl get ns\n</code></pre> <p>If your manager or you feel unhappy with the project, you can purge the namespace and the authorized certificate \ud83d\udc94 just with the following commands.</p> <pre><code>kubectl config set-context docker-desktop --user=docker-desktop # Set back to the K8s admin user\nkubectl config set-context docker-desktop --namespace=default # Set back to the default namespace\nkubectl delete csr compordept-project-env-admin\nkubectl delete ns compordept-project-env\n</code></pre> <p>Congratulations! \ud83c\udf89 You know how to set up and manage a multi-tenant K8s cluster organized by namespaces!</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#spark-on-k8s-via-cli","title":"\ud83d\udcbb Spark on K8s via CLI","text":"<p>In this part, we are going to install Spark and launch <code>spark-submit</code> directly on the previously set up K8s namespace!</p> Spark on K8s via CLI <p>Let's get to work!</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#installation_1","title":"Installation","text":"<p>Three prerequisites are needed to be able to launch Spark on K8s using the <code>spark-submit</code> command.</p> <ul> <li>\ud83d\udddd\ufe0f K8s credentials setup on the machine: Already done previously \ud83d\ude07!</li> <li>\u26a1 Spark installed on the machine executing the command</li> <li>\ud83d\udd27 A dedicated K8s service account for the Spark driver pod: This is necessary because it's the Spark driver pod who creates Spark executors and monitors them, manages necessary services, manages necessary configmaps and claims or releases volumes.</li> </ul> <p>To install Spark, here are the two necessary commands.</p> <ul> <li>Install Java 17 first using Coursier</li> </ul> <pre><code>curl -fL \"https://github.com/coursier/launchers/raw/master/cs-x86_64-pc-linux.gz\" | gzip -d &gt; cs\nchmod +x cs\n./cs setup -y --jvm 17\nrm cs\nsource ~/.profile\njava --version\n</code></pre> <ul> <li>Install Spark</li> </ul> <pre><code>mkdir -p ~/apps/spark\ncurl -fLo ~/apps/spark/spark-3.5.1-bin-hadoop3-scala2.13.tgz https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3-scala2.13.tgz\ntar -xf ~/apps/spark/spark-3.5.1-bin-hadoop3-scala2.13.tgz -C ~/apps/spark\nrm ~/apps/spark/spark-3.5.1-bin-hadoop3-scala2.13.tgz\necho -e '\\nexport PATH=\"~/apps/spark/spark-3.5.1-bin-hadoop3-scala2.13/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nspark-submit --version\n</code></pre> <p>Then you can install the Spark service account bound to <code>namespace-admin</code> role. This part assumes the isolated namespace is properly set up.</p> <pre><code>echo '\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: spark\nsubjects:\n  - kind: ServiceAccount\n    name: spark\nroleRef:\n  kind: Role\n  name: namespace-admin\n  apiGroup: rbac.authorization.k8s.io\n' | kubectl apply -f -\n</code></pre> <p>You can list service accounts to check if Spark's is correctly installed or not.</p> <pre><code>kubectl get sa\n</code></pre> <p>You can delete the service account with the following command.</p> <pre><code>kubectl delete rolebinding spark\nkubectl delete sa spark\n</code></pre> <p>Let's go to the next part! Submitting a Spark job! \ud83d\ude03</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#submitting-a-spark-job","title":"Submitting a Spark job","text":"<p>The goal is to use the provided Spark application with the installation: \"SparkPi\". By default, when building your own Spark application, you should build a custom Docker image following the official guide. It is also possible to use the Spark official Docker image on DockerHub as a base to inject your custom Spark application JAR. In our case, the Spark official Docker image already includes the \"SparkPi\" application at '/opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar', so we are going to use that directly.</p> <p>This part is going to be short because all the prerequisites are already done. It's now just a matter of launching the following command \ud83d\ude0e.</p> <pre><code>spark-submit --master k8s://https://kubernetes.docker.internal:6443 --deploy-mode cluster --name spark-app --class org.apache.spark.examples.SparkPi --conf spark.kubernetes.driver.request.cores=50m --conf spark.kubernetes.executor.request.cores=200m --conf spark.driver.memory=512m --conf spark.executor.memory=512m --conf spark.executor.instances=1 --conf spark.kubernetes.container.image=spark:3.5.1-scala2.12-java17-ubuntu --conf spark.kubernetes.namespace=compordept-project-env --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar 1\n</code></pre> <p>You can open a second terminal to watch the Spark pods in action h\u00e9h\u00e9 \ud83e\udd29.</p> <pre><code>kubectl get po -w\n</code></pre> <p>If you take a look at the driver pod logs, you should see the Pi estimate.</p> <pre><code>kubectl logs spark-app-4ac4bd9034099a45-driver\n</code></pre> <pre><code>Pi is roughly 3.139351393513935\n</code></pre> <p>The command to kill Spark driver pods if necessary.</p> <pre><code>spark-submit --kill compordept-project-env:spark-app* --master k8s://https://kubernetes.docker.internal:6443\n</code></pre> <p>Congratulations! You are now capable of running Spark on K8s and all without K8s cluster-level access, just namespace-level access is enough \ud83d\udc4d! It also means it's extremely easy to uninstall the project from the K8s cluster, just a matter of <code>kubectl delete ns compordept-project-env</code> \ud83d\ude09. But, it requires installing Spark on the machine executing the command as you can see.</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#spark-on-k8s-via-kubeflow","title":"\ud83d\udd27 Spark on K8s via Kubeflow","text":"<p>First, let's clean up some leftovers from the previous part.</p> <pre><code>spark-submit --kill compordept-project-env:spark-app* --master k8s://https://kubernetes.docker.internal:6443\nkubectl delete rolebinding spark\nkubectl delete sa spark\n</code></pre> <p>In this part, we are going to show that the Kubeflow Spark operator is not compatible with our previously set-up isolated namespace. Then, we will revert to cluster-level access, install the Kubeflow Spark operator, and launch the same 'SparkPi' application.</p> Spark on K8s via Kubeflow <p>Let's go!</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#installation_2","title":"Installation","text":"<p>Here are the steps to install Kubeflow Spark operator:</p> <ul> <li>\u2b07\ufe0f\ud83d\udee0\ufe0f Install Helm: Kubeflow Spark operator is available via a Helm chart, which is why the Helm CLI is necessary.</li> <li>\ud83d\udd11\ud83c\udf10 Ensure you have necessary Kubernetes cluster-level access: Kubeflow Spark operator installs CRDs and ClusterRoles, which require cluster-level access.</li> <li>\ud83d\udd27\ud83c\udf10 Add the Kubeflow Spark operator repository locally and install it on the Kubernetes cluster</li> </ul> <p>Let's make Kubeflow Spark operator work!</p> <p>Here is the command to install Helm.</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nrm ./get_helm.sh\n</code></pre> <p>You can confirm the installation with the following command:</p> <pre><code>helm version\n</code></pre> <pre><code>version.BuildInfo{Version:\"v3.15.1\", GitCommit:\"e211f2aa62992bd72586b395de50979e31231829\", GitTreeState:\"clean\", GoVersion:\"go1.22.3\"}\n</code></pre> <p>If it worked, Helm is perfectly installed \ud83d\ude01!</p> <p>Next, we have the necessary Kubernetes cluster-level access, but let's skip this part for now. Remember, we are currently set with namespace-level access using the <code>namespace-admin</code> role. So, let's try installing the Kubeflow Spark operator without cluster-level access.</p> <pre><code>helm repo add spark-operator https://kubeflow.github.io/spark-operator\nhelm install devops-spark-operator spark-operator/spark-operator --version 1.4.0 -n compordept-project-env\n</code></pre> <p>You should receive the following 'Forbidden' access error.</p> <pre><code>Error: INSTALLATION FAILED: failed to install CRD crds/sparkoperator.k8s.io_scheduledsparkapplications.yaml: 1 error occurred:\n        * customresourcedefinitions.apiextensions.k8s.io is forbidden: User \"namespace-admin\" cannot create resource \"customresourcedefinitions\" in API group \"apiextensions.k8s.io\" at the cluster scope\n</code></pre> <p>As you can see, it's not possible to use Spark via Kubeflow Spark Operator without Kubernetes cluster-level access! Let's switch back to our Kubernetes admin user.</p> <pre><code>kubectl config set-context docker-desktop --user=docker-desktop # Set back to the K8s admin user\nkubectl config delete-user compordept-project-env-admin\nkubectl delete rolebinding namespace-admin\nkubectl delete role namespace-admin\nkubectl delete csr compordept-project-env-admin\n</code></pre> <p>Now let's try again.</p> <pre><code>helm install devops-spark-operator spark-operator/spark-operator --version 1.4.0 -n compordept-project-env\n</code></pre> <pre><code>NAME: devops-spark-operator\nLAST DEPLOYED: Fri Jun 21 07:48:49 2024\nNAMESPACE: compordept-project-env\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>Ok, it seems it has worked now. But if you look at the pods, the Kubeflow Spark operator pod is missing.</p> <pre><code>kubectl get po\n</code></pre> <pre><code>No resources found in compordept-project-env namespace.\n</code></pre> <p>This is due to the <code>ResourceQuota</code> in place, which requires requests and limits to be set up on all pods in order to orchestrate them. I did not find a way to configure them during Helm chart installation. So let's just remove the quota:</p> <pre><code>kubectl delete quota namespace-quota\n</code></pre> <p>Then, try to reinstall the chart.</p> <pre><code>helm uninstall devops-spark-operator -n compordept-project-env\nhelm install devops-spark-operator spark-operator/spark-operator --version 1.4.0 -n compordept-project-env\n</code></pre> <p>Then, list the pods.</p> <pre><code>kubectl get po\n</code></pre> <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\ndevops-spark-operator-76968c6d75-nn4l7   1/1     Running   0          84s\n</code></pre> <p>Wow, this time it actually worked \ud83d\udc4d! Installation complete!</p> <p>Here are some commands if you would like to delete the created resources.</p> <pre><code>helm uninstall devops-spark-operator -n compordept-project-env\nkubectl delete crds scheduledsparkapplications.sparkoperator.k8s.io sparkapplications.sparkoperator.k8s.io\n</code></pre> <p>Let's proceed to submitting Spark applications now.</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#submitting-a-spark-job_1","title":"Submitting a Spark job","text":"<p>To launch a Spark job, here are the steps:</p> <ul> <li>\ud83d\udcc4\u2699\ufe0f Define a K8s manifest YAML file using the resource <code>SparkApplication</code>. This resource is not native to K8s; it comes from the Kubeflow Spark Operator CRDs.</li> <li>\ud83d\udcc4\ud83d\ude80 Then, just apply the K8s file.</li> </ul> <p>Let's use the official Spark Docker image that comes with the \"SparkPi\" application, like when we were running Spark via CLI.</p> <pre><code>echo \"\napiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: SparkApplication\nmetadata:\n  name: spark-app\nspec:\n  type: Scala\n  mode: cluster\n  image: \"spark:3.5.1\"\n  imagePullPolicy: Always\n  mainClass: org.apache.spark.examples.SparkPi\n  mainApplicationFile: \"local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar\"\n  sparkVersion: \"3.5.1\"\n  restartPolicy:\n    type: Never\n  driver:\n    cores: 1\n    memory: \"512m\"\n    labels:\n      version: 3.5.1\n    serviceAccount: devops-spark-operator-spark\n  executor:\n    cores: 1\n    instances: 1\n    memory: \"512m\"\n    labels:\n      version: 3.5.1\n\" | kubectl apply -f -\n</code></pre> <p>If you pay attention, in terms of driver and executor core requests, it's not exactly the same as the previous Spark submit via Spark CLI. Previously, '50m' and '200m' were set. This is because the <code>SparkApplication</code> resource does not support millicore units.</p> <p>The <code>SparkApplication</code> resource, like any other K8s resource, can be listed.</p> <pre><code>kubectl get sparkapp\n</code></pre> <pre><code>NAME        STATUS      ATTEMPTS   START                  FINISH       AGE\nspark-app   SUBMITTED   1          2024-06-22T07:27:38Z   &lt;no value&gt;   8s\n</code></pre> <p>If you list the pods, you will see your Spark application running!</p> <pre><code>kubectl get pod\n</code></pre> <pre><code>NAME                                     READY   STATUS              RESTARTS      AGE\ndevops-spark-operator-76968c6d75-nn4l7   1/1     Running             2 (57m ago)   25h\nspark-app-driver                         1/1     Running             0             35s\nspark-pi-5672d5903ed86ce8-exec-1         0/1     ContainerCreating   0             1s\n</code></pre> <p>And you can get your estimated Pi just like for the Spark via CLI.</p> <pre><code>kubectl logs spark-app-driver\n</code></pre> <pre><code>Pi is roughly 3.1459357296786483\n</code></pre> <p>The following command deletes <code>SparkApplication</code> resources and their related pods.</p> <pre><code>kubectl delete sparkapp spark-app\n</code></pre> <p>Congratulations! You know how to launch a Spark job using the Kubeflow Spark operator! The advantage of using the Kubeflow Spark operator instead of the default Spark via CLI is not needing to set up Spark locally or install Java and Spark. Plus, the Kubeflow Spark operator offers other features like cron scheduling and the <code>sparkctl</code> command-line tool.</p>"},{"location":"2024/06/22/mastering-spark-on-k8s--and-why-i-dumped--kubeflow-spark-operator-formerly-googles-spark-operator/#conclusion","title":"\ud83d\udd1a Conclusion","text":"<p>I think you don't need the Kubeflow Spark operator. The basic Spark via CLI for running jobs on Kubernetes is best, even if you have cluster-level access to install the Kubeflow Spark operator. This way, it enforces data engineers to work with tight namespace-level access on K8s, which is often the case in the industry.</p> <p>I write monthly on the LovinData Blog and on Medium, and like to give back the knowledge I've learned. So don't hesitate to reach out; I'm always available to chat about nerdy stuff \ud83e\udd17! Here are my socials: LinkedIn, Twitter and Reddit. Otherwise, let's learn together in the next story \ud83e\udee1! Bye \u2764\ufe0f.</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/","title":"From AWS to DIY: Building a Cost-Effective \ud83d\udcb8 Home Server with Ubuntu Server, Docker, Portainer & Nginx on a High-Performance Mini PC! \ud83d\ude80","text":"<p>As a software engineer, I\u2019ve relied on AWS for cloud computing for some time, but the rising costs finally pushed me to rethink things \ud83d\udcb8. During Black Friday, I jumped on a deal I couldn\u2019t resist \ud83c\udf89 and built a home setup around a GMKtec mini PC with an AMD Ryzen 7 8845HS, paired with 2 x 48GB of DDR5 5600MHz Crucial RAM and two 4TB Samsung 990 PRO PCIe 4.0 NVMe M.2 SSDs. The whole setup cost me \u20ac1,100 (about $1,200 USD) and runs at only 35W \u26a1\u2014that\u2019s roughly \u20ac4.30 ($4.60 USD) a month in electricity here in France \ud83c\uddeb\ud83c\uddf7. Compare that to the $517 per month I\u2019d pay to run an AWS EC2 m8g.4xlarge instance. Now, I\u2019ve got 16 CPUs (8 cores, 16 threads) \ud83d\udcbb, 96GB of speedy RAM \u2699\ufe0f, and 8TB of PCIe 4.0 NVMe storage \ud83d\udcbe for demanding workloads. It\u2019s a massive money-saver \ud83d\udcb0 and the perfect base for a home lab running Ubuntu Server \ud83d\udc27. Tools like Portainer make container management easy \ud83d\udee0\ufe0f, and Nginx Proxy Manager simplifies reverse proxy configurations \ud83d\udd04. If cloud costs are draining your budget, making the switch is well worth it \ud83d\ude80!</p> <p></p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#ubuntu-server-docker-portainer-nginx-proxy-manager","title":"\ud83e\udd14 Ubuntu Server, Docker, Portainer &amp; Nginx Proxy Manager?","text":"<p>What are Ubuntu Server, Docker, Portainer, and Nginx Proxy Manager? Quickly:</p> <ul> <li>\ud83d\udc27 Ubuntu Server: Ubuntu Server is a robust and popular Linux-based operating system designed for server environments, providing the foundation for building reliable and secure web applications and services.</li> <li>\ud83d\udc33 Docker: Docker is a platform that allows developers to automate the deployment of applications inside lightweight, portable containers, simplifying environment management and ensuring consistency across different systems.</li> <li>\ud83d\udee0\ufe0f Portainer: Portainer is a simple and easy-to-use management interface for Docker, providing a graphical dashboard for managing containers, images, and volumes, which helps streamline container operations for developers.</li> <li>\ud83d\udd04 Nginx Proxy Manager: Nginx Proxy Manager is an intuitive tool for managing Nginx proxy configurations, allowing users to easily set up reverse proxies, SSL certificates, and routing to their applications, all through a user-friendly interface.</li> </ul> <p>At the end of this guide, you should be able to set up a home server environment with Ubuntu Server, Docker, Portainer, and Nginx Proxy Manager, enabling efficient management of containers and web traffic. Let's get started!</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#create-a-bootable-usb-key","title":"\ud83d\udd0c Create a Bootable USB Key","text":"<p>In this part, we are going to create a bootable USB key using Rufus.</p> Create a bootable USB key with Rufus <p>The goal is to set up a USB drive that will allow us to easily install Ubuntu Server on a system.</p> <p>Go to rufus.ie to download and install Rufus.</p> Install Rufus <p>Do the same for Ubuntu Server at ubuntu.com/download/server.</p> Download Ubuntu Server ISO <p>Now it's time to create a bootable USB key with the Ubuntu Server ISO. Plug in your USB key, open Rufus, and for <code>Boot selection</code>, select the downloaded Ubuntu Server ISO. Here\u2019s what your configuration should look like:</p> Rufus Configuration <p>Click on <code>START</code>, and now all you have to do is wait \ud83d\udc4d. Congratulations, you've set up a bootable USB key! Let's use it to install Ubuntu Server on our machine!</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#install-ubuntu-server-os","title":"\ud83d\udc27 Install Ubuntu Server OS","text":"<p>The goal now is to use the USB key containing our Ubuntu Server ISO to install Ubuntu Server on our machine.</p> Install Ubuntu Server OS <p>The ultimate goal is to have a machine that we can connect to via SSH, just like one rented from any cloud provider \ud83d\ude09.</p> <p>Start by plugging the USB key, then turn ON the machine and open the BIOS. To open the BIOS, restart your PC and press the designated key (commonly F2, F12, Delete, or Esc), which depends on your PC's manufacturer.</p> The USB key is set as the first boot option <p>As explained on the screen, the USB key is set as the first boot option. Then restart the machine, you should be welcomed with the choice: <code>Try or install Ubuntu Server</code>.</p> Try or install Ubuntu Server <p>Select this choice, and then you will follow a series of instructions. Here are some tricky parts (obvious parts will not be detailed; regarding the few screens that will appear, I am not the original author. If you wish for a more detailed explanation, please check out SavvyNik's video \ud83d\udc4d).</p> <ul> <li>Choose the basic installation.</li> </ul> Choose the basic installation <ul> <li>Set up the internet connection via Ethernet or WiFi.</li> </ul> Network connections <p>On your side, you may have more interfaces. It can also be via WiFi \ud83d\udc4d! The important thing is to have one set up because internet will be needed for package downloads and updates, and, of course, for the SSH connection through the home network.</p> <ul> <li><code>Use an entire disk</code> and <code>Set up this disk as LVM group</code>.</li> </ul> Guided storage configuration <ul> <li>Set the storage configuration to utilize all the disk space.</li> </ul> <p>For the storage configuration part, by default, it does not utilize all the disk space. You can see this in the <code>free space</code> field in the <code>DEVICE</code> section:</p> Storage configuration - Before <p>So the goal is to allocate all this unconfigured free space to <code>ubuntu-lv</code>. This will allow you to utilize all your disk space for your files, packages, etc.</p> Storage configuration - Editing logical volumne ubuntu-lv of ubuntu-vg Storage configuration - After <ul> <li>Install OpenSSH server.</li> </ul> Install OpenSSH server <ul> <li>Regarding <code>Featured Server Snaps</code>, do not select anything and select <code>Done</code>; it will start installing packages, be patient, and then just click <code>Reboot Now</code>.</li> </ul> Install complete! <ul> <li>You can now login!</li> </ul> <p>Let it boot, and you should encounter an error because the USB key is still plugged in, and the server tries to boot from it. Turn off the server, remove the USB key, and then boot up the server. If you end up with the following screen, congratulations, you've successfully installed Ubuntu Server \ud83d\ude0d!</p> You can now login! <p>Final thing, let's check if we can connect to the server from another machine via SSH. First, find the server's IP by logging in and running the following command on the server:</p> From server<pre><code>ip a\n</code></pre> <p>It should give you a list of interfaces. Find the one that has an <code>inet</code> address formatted as <code>192.168.1.X</code>:</p> From server<pre><code>...\n4: wlp4s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether xx:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.X/24 metric 600 brd 192.168.1.255 scope global dynamic wlp4s0\n       valid_lft 38554sec preferred_lft 38554sec\n    inet6 xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:xxxx/64 scope global dynamic mngtmpaddr noprefixroute\n       valid_lft 86165sec preferred_lft 86165sec\n    inet6 fe80::xxxx:xxxx:xxxx:xxxx/64 scope link\n       valid_lft forever preferred_lft forever\n...\n</code></pre> <p>Let's connect via SSH.</p> From work machine<pre><code>ssh myuser@192.168.1.X\n</code></pre> <p>If it connects:</p> From work machine<pre><code>...\nLast login: Mon Jan 13 05:49:37 2025 from 192.168.1.Y\nmyuser@myserver:~$\n</code></pre> <p>Then, congratulations! You've successfully set up a server similar to the ones you can rent from AWS or any other cloud provider \ud83e\udd29. All the following commands will be executed from the work machine on behalf of the server via SSH from now on!</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#set-up-docker-portainer-and-nginx-proxy-manager","title":"\ud83d\udc0b Set Up Docker, Portainer, and Nginx Proxy Manager","text":"<p>Now, it's time to set up all the necessary tools to deploy, maintain, and expose our services/applications: Docker, Portainer, and Nginx Proxy Manager.</p> Docker, Portainer, and Nginx Proxy Manager <p>Let's first install Docker (official link, if necessary):</p> <pre><code># Add Docker's official GPG key\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n</code></pre> <pre><code># Install Docker\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>You can verify the Docker installation using the following command:</p> <pre><code>sudo docker version\ndocker compose version\n</code></pre> <pre><code>Client: Docker Engine - Community\n Version:           27.4.1\n API version:       1.47\n Go version:        go1.22.10\n Git commit:        b9d17ea\n Built:             Tue Dec 17 15:45:46 2024\n OS/Arch:           linux/amd64\n Context:           default\n\nServer: Docker Engine - Community\n Engine:\n  Version:          27.4.1\n  API version:      1.47 (minimum version 1.24)\n  Go version:       go1.22.10\n  Git commit:       c710b88\n  Built:            Tue Dec 17 15:45:46 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.7.24\n  GitCommit:        88bf19b2105c8b17560993bee28a01ddc2f97182\n runc:\n  Version:          1.2.2\n  GitCommit:        v1.2.2-0-g7cb3632\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\nDocker Compose version v2.32.1\n</code></pre> <p>Then install Portainer (official link, if necessary):</p> <pre><code># Create Portainer volume\nsudo docker volume create portainer_data\n\n# Pull &amp; Run Portainer\nsudo docker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:2.25.1\n</code></pre> <p>Once completed, you can navigate to https://192.168.1.X:9443. You should be prompted to create the administrator account. Once completed and logged in, the following screen should be presented to you:</p> Portainer ready! <p>Congratulations, you've successfully installed Portainer! You can now deploy, maintain, and monitor containerized applications through a Web UI.</p> <p>Finally, let's install Nginx Proxy Manager (official link, if necessary):</p> <pre><code># Create Nginx Proxy Manager volumes\nsudo docker volume create nginx_proxy_manager_data\nsudo docker volume create nginx_proxy_manager_etc_letsencrypt\n\n# Pull &amp; Run Nginx Proxy Manager\nsudo docker run -d -p 80:80 -p 443:443 -p 81:81 --name nginx_proxy_manager --restart=always -v nginx_proxy_manager_data:/data -v nginx_proxy_manager_etc_letsencrypt:/etc/letsencrypt jc21/nginx-proxy-manager:2.12.2\n</code></pre> <p>Once completed, you can navigate to http://192.168.1.X:81. The login and password are <code>admin@example.com</code> and <code>changeme</code>. Once logged in, you will be prompted to change these parameters and will be welcomed with the following screen:</p> Nginx Proxy Manager ready! <p>Congratulation! you've successfully installed Nginx Proxy Manager! \ud83e\udd17</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#expose-your-services-to-the-internet-securely","title":"\ud83c\udf10 Expose Your Services to the Internet Securely","text":"<p>In this section, we are going to expose Nginx Proxy Manager and Portainer to the outside world. The goal is to be able to manage, deploy, and maintain our services from anywhere! In this part, depending on your ISP and domain name provider, it's highly likely that you do not have exactly the same screens.</p> Exposing Nginx Proxy Manager and Portainer to the outside world <p>Warning</p> <p>Beware, exposing Nginx Proxy Manager and Portainer allows people to attempt to crack your login/password. If they succeed, it means they have control over the deployed applications. So, if working from home is the only thing you do, it might be wise not to expose these two services and to only access them from your home network!</p> <p>Let's start by forwarding ports 80 and 443 requests from our ISP router to our server's ports 80 and 443:</p> Forwarding ports 80 and 443 requests <p>Let's now get the CNAME or router's Internet IP:</p> Forwarding ports 80 and 443 requests <p>Note</p> <p>You can also go to sites like whatismyip.com to get your IP.</p> <p>Now let's buy a domain name. In my case I choosed namecheap.com. After buying the domain name, let's configure it to be routed to our ISP router.</p> Domain routing <p>Note</p> <p>You can also route to your ISP router using the IP address instead of the CNAME.</p> <p>Note</p> <p>Host means the subdomain name. For example, with a host of <code>nginx</code>, it will route the domain <code>nginx.mydomain.topdomain</code> to your ISP router.</p> <p>Let's go back to our home network and configure Nginx to allow requests from these domains to our services. We will use the example of Nginx Proxy Manager itself:</p> Configure your \"Let's Encrypt\" SSL certificate for your domain \"Add Proxy Host\" to the corresponding service with \"Force SSL\" and the corresponding SSL certificate <p>The same steps can be applied to Portainer, but be sure to use <code>https</code> as the <code>Scheme</code> when adding the proxy host, because the Portainer service uses https by default.</p> <p>Going to \"nginx.mydomain.topdomain\" and \"portainer.mydomain.topdomain\", you should be able to access your two services! Congratulations, you've learned how to expose your services to the Internet! \ud83c\udf89</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#example-use-case-open-webui-and-ollama-setup","title":"\ud83c\udfaf Example Use Case: Open WebUI and Ollama Setup","text":"<p>This section aims to demonstrate how to properly install a stack of containers for a given application and expose it to the Internet. LLMs are hot topics nowadays, so let's use the Open WebUI and Ollama stack as an example.</p> Open WebUI and Ollama Setup <p>Let's start by running the LLM container stack on our server. Here are the steps:</p> <ul> <li>Navigate to the Portainer home page.</li> <li>Select the <code>Local</code> environment, go to <code>Stacks</code>, and click on <code>Add stack</code>.</li> <li>Input a <code>Name</code> for the stack, for example, <code>llm</code>.</li> <li>Select <code>Web editor</code> and paste the following Docker Compose file.</li> </ul> <pre><code>services:\n  # https://github.com/open-webui/open-webui\n  openwebui:\n    image: ghcr.io/open-webui/open-webui:v0.5.4\n    restart: unless-stopped\n    ports:\n      - ${OPENWEBUI_PORT}:8080\n    environment:\n      OLLAMA_API_BASE_URL: http://ollama:${OLLAMA_PORT}\n    volumes:\n      - openwebui_app_backend_data:/app/backend/data\n    healthcheck:\n      test: \"curl -f http://localhost:8080\"\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    depends_on:\n      ollama:\n        condition: service_healthy\n\n  # https://hub.docker.com/r/ollama/ollama/tags\n  ollama:\n    image: ollama/ollama:0.5.4\n    restart: unless-stopped\n    ports:\n      - ${OLLAMA_PORT}:11434\n    volumes:\n      - ollama_root_ollama:/root/.ollama\n    healthcheck:\n      test: \"ollama --version &amp;&amp; ollama ps || exit 1\" # https://github.com/ollama/ollama/issues/1378#issuecomment-2436650823\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\nvolumes:\n  openwebui_app_backend_data:\n    driver: local\n  ollama_root_ollama:\n    driver: local\n</code></pre> <ul> <li>In the <code>Environment variables</code> section, add the following environment variables: <code>OPENWEBUI_PORT -&gt; 11435</code> and <code>OLLAMA_PORT -&gt; 11434</code>.</li> </ul> <p>You should end up with a configuration that looks like this:</p> Configuring the Open WebUI and Ollama stack <p>Let's now expose our application to the outside world. To be precise, it means exposing the Open WebUI service. This part is identical to when Nginx Proxy Manager and Portainer were exposed, so I invite you to check the previous part. The important part is to expose only the Open WebUI and enable <code>Websockets Support</code> because text writing on the fly is done through a websocket. After that, you should be able to navigate to 'llm.mydomain.topdomain', and by configuring your admin account (mandatory on first page load), you should end up on the following page:</p> Open WebUI <p>Let's now download the latest Phi-4 with its 14 billion parameters, rumored to rival OpenAI's GPT-4o mini, which everyone is talking about. You can also try other LLMs instead; models are available in the Ollama Models section.</p> The Phi-4 model Downloading the Phi-4 model <p>Here is an example result of prompting:</p> An example of a prompt result <p>The latest Phi is running on our server, though it's currently best suited for background tasks due to its slower performance. \ud83c\udf89 Congratulations! You've successfully self-hosted your own private LLM platform! \ud83d\ude80</p>"},{"location":"2025/01/01/from-aws-to-diy-building-a-cost-effective--home-server-with-ubuntu-server-docker-portainer--nginx-on-a-high-performance-mini-pc-/#whats-next","title":"\ud83d\udd2e What's next?","text":"<p>My thoughts are that self-hosting is ideal for ephemeral computation or data. It's great because, as we can see, it\u2019s more than doable, and when compared to cloud prices, it's less costly. However, running a full business on a self-hosted lab isn't ideal. My home isn\u2019t \"secure\". So, if user data is critical to the application, I would rent persistent services like RDS, S3, etc. But for all the application logic and computation, since it's ephemeral, I would go the self-hosting route.</p> <p>Regarding the technical/hardware side, I might buy another mini PC to set up a K8S cluster. Alternatively, I\u2019m considering the new Intel Arc Battlemage GPUs that just got released. The GMKTec AMD Ryzen 7 8845HS Mini PC\u2014NucBox K8 Plus I have even has an Oculink port, h\u00e9h\u00e9! Or maybe I\u2019ll go for both options\u2014imagine a K8S cluster with GPU enabled on each node \ud83e\udd24. We\u2019ll see!</p> <p>I try to write monthly on the LovinData Blog and on Medium, and like to give back the knowledge I've learned. So don't hesitate to reach out; I'm always available to chat about nerdy stuff \ud83e\udd17! Here are my socials: LinkedIn, Twitter and Reddit. Otherwise, let's learn together in the next story \ud83e\udee1! Bye \u2764\ufe0f.</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/linux/","title":"Linux","text":""},{"location":"category/docker/","title":"Docker","text":""},{"location":"category/nginx/","title":"Nginx","text":""},{"location":"category/devops/","title":"DevOps","text":""},{"location":"category/kubernetes/","title":"Kubernetes","text":""},{"location":"category/data-engineering/","title":"Data Engineering","text":""},{"location":"category/typescript/","title":"TypeScript","text":""},{"location":"category/scala/","title":"Scala","text":""},{"location":"category/frontend/","title":"Frontend","text":""},{"location":"category/backend/","title":"Backend","text":""},{"location":"category/python/","title":"Python","text":""}]}